{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d77e911",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import clip\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "from dataloaders import CustomDataLoader\n",
    "from custom_tokenizers import Tokenizer\n",
    "from types import SimpleNamespace\n",
    "from decoder_layers import KVCache, PaliGemmaForConditionalGeneration\n",
    "from configs.config import DataConfig, EncoderConfig, DecoderConfig, PaliGemmaConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54211390",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    pixel_values,        # shape: (B, 2, C, H, W)\n",
    "    image_token_index,   # placeholder index for image tokens\n",
    "    max_length=30,\n",
    "    num_beams=3,\n",
    "    eos_token_id=2,\n",
    "    pad_token_id=0,\n",
    "    device=\"cpu\",\n",
    "):\n",
    "    model.eval()\n",
    "    batch_size = pixel_values.size(0)\n",
    "    eos_token_id = eos_token_id or tokenizer.eos_token_id\n",
    "\n",
    "    # Encode images\n",
    "    B, N, C, H, W = pixel_values.shape\n",
    "    pixel_values = pixel_values.view(B * N, C, H, W).to(device)\n",
    "    vision_features = model.vision_tower(pixel_values)\n",
    "    vision_features = vision_features.view(B, N, *vision_features.shape[1:])\n",
    "    image_features = torch.cat([vision_features[:, 0], vision_features[:, 1]], dim=1)\n",
    "    image_features = model.multi_modal_projector(image_features)  # shape: [B, Seq, Hidden]\n",
    "\n",
    "    # Init input_ids with image token index\n",
    "    input_ids = torch.full((batch_size * num_beams, 1), image_token_index, dtype=torch.long, device=device)\n",
    "    attention_mask = torch.ones_like(input_ids, device=device)\n",
    "\n",
    "    # Expand inputs for each beam\n",
    "    image_features = image_features.unsqueeze(1).repeat(1, num_beams, 1, 1)\n",
    "    image_features = image_features.view(batch_size * num_beams, *image_features.shape[2:])\n",
    "\n",
    "    beam_scores = torch.zeros((batch_size, num_beams), device=device)\n",
    "    beam_scores[:, 1:] = -1e9  # mask beams other than first\n",
    "    beam_scores = beam_scores.view(-1)  # shape: [B * num_beams]\n",
    "\n",
    "    sequences = input_ids\n",
    "    is_done = [False] * batch_size\n",
    "\n",
    "    for step in range(max_length):\n",
    "        # Embedding\n",
    "        input_embeds = model.language_model.get_input_embeddings()(sequences)\n",
    "        \n",
    "        # Merge image + text features\n",
    "        merged_input, attn_mask, pos_ids = model._merge_input_ids_with_image_features(\n",
    "            sequences, image_features, input_embeds, attention_mask, kv_cache=None\n",
    "        )\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model.language_model(\n",
    "            inputs_embeds=merged_input,\n",
    "            attention_mask=attn_mask,\n",
    "            position_ids=pos_ids,\n",
    "        )\n",
    "        logits = outputs[\"logits\"]  # shape: [B * num_beams, Seq_len, Vocab]\n",
    "        next_token_logits = logits[:, -1, :]  # take last token only\n",
    "        next_token_log_probs = F.log_softmax(next_token_logits, dim=-1)\n",
    "\n",
    "        # Add current beam scores\n",
    "        next_token_log_probs = next_token_log_probs + beam_scores[:, None]\n",
    "\n",
    "        # Get top k * num_beams candidates\n",
    "        vocab_size = next_token_log_probs.size(-1)\n",
    "        next_token_log_probs = next_token_log_probs.view(batch_size, num_beams * vocab_size)\n",
    "        topk_log_probs, topk_indices = torch.topk(next_token_log_probs, num_beams, dim=-1)\n",
    "\n",
    "        # Prepare for next step\n",
    "        beam_indices = topk_indices // vocab_size\n",
    "        token_indices = topk_indices % vocab_size\n",
    "\n",
    "        # Reorder sequences and image features\n",
    "        sequences = sequences.view(batch_size, num_beams, -1)\n",
    "        new_sequences = []\n",
    "        for i in range(batch_size):\n",
    "            new_sequences.append(sequences[i, beam_indices[i]])\n",
    "        sequences = torch.stack(new_sequences).view(batch_size * num_beams, -1)\n",
    "        sequences = torch.cat([sequences, token_indices.view(-1, 1)], dim=-1)\n",
    "\n",
    "        # Update scores\n",
    "        beam_scores = topk_log_probs.view(-1)\n",
    "\n",
    "        # Update attention mask\n",
    "        attention_mask = torch.cat([attention_mask, torch.ones_like(token_indices.view(-1, 1))], dim=1)\n",
    "\n",
    "        # Check if all sequences have ended\n",
    "        if eos_token_id is not None:\n",
    "            for i in range(batch_size):\n",
    "                for beam_id in range(num_beams):\n",
    "                    if sequences[i * num_beams + beam_id, -1] == eos_token_id:\n",
    "                        is_done[i] = True\n",
    "\n",
    "        if all(is_done):\n",
    "            break\n",
    "\n",
    "    # Reshape to [batch_size, num_beams, seq_len] and pick best beam\n",
    "    sequences = sequences.view(batch_size, num_beams, -1)\n",
    "    beam_scores = beam_scores.view(batch_size, num_beams)\n",
    "    best_indices = torch.argmax(beam_scores, dim=1)\n",
    "\n",
    "    best_sequences = []\n",
    "    for i in range(batch_size):\n",
    "        best_sequences.append(sequences[i, best_indices[i]])\n",
    "    best_sequences = torch.stack(best_sequences)\n",
    "\n",
    "    return best_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8f75798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overlies . desired nipple medial decrease obscuring correction ectatic combination\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "config = PaliGemmaConfig(text_config=DecoderConfig(), vision_config=EncoderConfig())\n",
    "model = PaliGemmaForConditionalGeneration(config)\n",
    "model.load_state_dict(torch.load(\"checkpoints/best_model.pt\", map_location=\"cpu\"))\n",
    "model.eval()\n",
    "\n",
    "tokenizer = Tokenizer(DataConfig())\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_clip, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# Load and preprocess image\n",
    "image_1 = preprocess(Image.open(\"data/updated_iu_xray/CXR1_1_IM-0001/0.png\"))\n",
    "image_2 = preprocess(Image.open(\"data/updated_iu_xray/CXR1_1_IM-0001/1.png\"))\n",
    "pixel_values = torch.stack((image_1, image_2), 0).unsqueeze(0)\n",
    "generated_ids = beam_search_generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    pixel_values=pixel_values,\n",
    "    image_token_index=config.image_token_index,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    device=\"cpu\",\n",
    "    num_beams=5,\n",
    "    max_length=50,\n",
    ")\n",
    "\n",
    "# Decode to text\n",
    "captions = tokenizer.decode_batch(generated_ids)\n",
    "for caption in captions:\n",
    "    print(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59c4bdd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[978,   4, 407, 897, 843, 376, 932, 350, 466, 305]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_ids"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
