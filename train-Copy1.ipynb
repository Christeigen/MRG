{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53852407",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "754e3a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-01 22:34:58.630483: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-01 22:34:58.642040: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1751409298.655266   11475 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1751409298.659097   11475 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1751409298.670186   11475 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751409298.670204   11475 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751409298.670206   11475 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751409298.670208   11475 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-01 22:34:58.674037: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from transformers import get_scheduler\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from custom_tokenizers import Tokenizer\n",
    "from configs.config import DataConfig, EncoderConfig, DecoderConfig, PaliGemmaConfig\n",
    "from decoder_coba import KVCache, PaliGemmaForConditionalGeneration\n",
    "from dataloaders_coba import CustomDataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf9671bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model,\n",
    "    dataloader,\n",
    "    optimizer,\n",
    "    device,\n",
    "    epoch,\n",
    "    bert,\n",
    "    grad_accumulation_steps=1,\n",
    "    max_grad_norm=1.0,\n",
    "    use_amp=False,\n",
    "):\n",
    "    model.train()\n",
    "    kv_cache = KVCache()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "    config = DecoderConfig()\n",
    "\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for step, batch in enumerate(tqdm(dataloader, desc=f\"Epoch {epoch}\")):\n",
    "        input_ids, pixel_values, target, attention_mask, input_embeds = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        pixel_values = pixel_values.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        input_embeds = input_embeds.unsqueeze(1).to(device)\n",
    "        with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                pixel_values=pixel_values,\n",
    "                attention_mask=attention_mask,\n",
    "                inputs_embeds=input_embeds\n",
    "            )\n",
    "            \n",
    "            logits = outputs[\"logits\"]\n",
    "            shift_logits = logits[:, :-1, :].contiguous()\n",
    "            shift_labels = input_ids[:, 1:].contiguous()\n",
    "            \n",
    "            loss = torch.nn.functional.cross_entropy(\n",
    "                shift_logits.view(-1, config.vocab_size),\n",
    "                shift_labels.view(-1),\n",
    "                label_smoothing=0.1\n",
    "            )\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if (step + 1) % grad_accumulation_steps == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, dataloader, device, bert, use_amp=False):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "    config = DecoderConfig()\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Validating\"):\n",
    "        input_ids, pixel_values, target, attention_mask, input_embeds = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        pixel_values = pixel_values.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        input_embeds = input_embeds.unsqueeze(1).to(device)\n",
    "        with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                pixel_values=pixel_values,\n",
    "                attention_mask=attention_mask,\n",
    "                inputs_embeds=input_embeds\n",
    "            )\n",
    "            logits = outputs[\"logits\"]  # shape: [B, T, V]\n",
    "            shift_logits = logits[:, :-1, :].contiguous()\n",
    "            shift_labels = input_ids[:, 1:].contiguous()\n",
    "            \n",
    "            loss = torch.nn.functional.cross_entropy(\n",
    "                shift_logits.view(-1, config.vocab_size),\n",
    "                shift_labels.view(-1),\n",
    "                label_smoothing=0.1\n",
    "            )\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c993f54d-10a4-468c-965f-38083bd1666e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing embeddings for 2069 samples in split 'train'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2069 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "100%|██████████| 2069/2069 [00:07<00:00, 260.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved train embeddings to 'cached_embeds/train_bert_embeds.pt'\n",
      "Computing embeddings for 296 samples in split 'val'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/296 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "100%|██████████| 296/296 [00:01<00:00, 264.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved val embeddings to 'cached_embeds/val_bert_embeds.pt'\n",
      "Computing embeddings for 590 samples in split 'test'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/590 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "100%|██████████| 590/590 [00:02<00:00, 266.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved test embeddings to 'cached_embeds/test_bert_embeds.pt'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from configs.config import DataConfig\n",
    "from tqdm import tqdm\n",
    "\n",
    "def clean_report_iu_xray(report):\n",
    "    report_cleaner = lambda t: t.replace('..', '.').replace('1. ', '') \\\n",
    "        .replace('. 2. ', '. ').replace('. 3. ', '. ').replace('. 4. ', '. ').replace('. 5. ', '. ') \\\n",
    "        .replace(' 2. ', '. ').replace(' 3. ', '. ').replace(' 4. ', '. ').replace(' 5. ', '. ') \\\n",
    "        .strip().lower().split('. ')\n",
    "    sent_cleaner = lambda t: re.sub('[.,?;*!%^&_+():-\\[\\]{}]', '', t.replace('\"', '').replace('/', '')\n",
    "                                    .replace('\\\\', '').replace(\"'\", '').strip().lower())\n",
    "    tokens = [sent_cleaner(sent) for sent in report_cleaner(report) if sent_cleaner(sent) != []]\n",
    "    return ' . '.join(tokens) + ' .'\n",
    "\n",
    "def compute_and_save_bert_embeddings(split=\"train\", save_path=\"cached_bert_embeds.pt\"):\n",
    "    # === Load BERT model ===\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "    model = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\").eval().cuda()\n",
    "\n",
    "    # === Load annotations ===\n",
    "    config = DataConfig()\n",
    "    annotation_path = config.annotation_path\n",
    "    with open(annotation_path, 'r') as f:\n",
    "        data = json.load(f)[split]\n",
    "\n",
    "    print(f\"Computing embeddings for {len(data)} samples in split '{split}'...\")\n",
    "\n",
    "    embeddings = []\n",
    "    for example in tqdm(data):\n",
    "        report = clean_report_iu_xray(example[\"report\"])\n",
    "\n",
    "        # Tokenize\n",
    "        inputs = tokenizer(report, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            last_hidden_state = outputs.last_hidden_state\n",
    "            attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "            # Average the hidden states (excluding padding)\n",
    "            masked = last_hidden_state * attention_mask.unsqueeze(-1)\n",
    "            mean_embedding = masked.sum(dim=1) / attention_mask.sum(dim=1, keepdim=True)\n",
    "\n",
    "        # Save as 1D CPU tensor (remove batch dimension)\n",
    "        embeddings.append(mean_embedding.squeeze(0).cpu())\n",
    "\n",
    "    # === Save all embeddings ===\n",
    "    os.makedirs(\"cached_embeds\", exist_ok=True)\n",
    "    torch.save(embeddings, os.path.join(\"cached_embeds\", f\"{split}_bert_embeds.pt\"))\n",
    "    print(f\"Saved {split} embeddings to 'cached_embeds/{split}_bert_embeds.pt'\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    compute_and_save_bert_embeddings(split=\"train\")\n",
    "    compute_and_save_bert_embeddings(split=\"val\")\n",
    "    compute_and_save_bert_embeddings(split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58d00b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Hyperparameters\n",
    "    epochs = 10\n",
    "    batch_size = 64\n",
    "    learning_rate = 3e-5\n",
    "    grad_accumulation_steps = 1\n",
    "    use_amp = True\n",
    "    patience = 30\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    # === Load Config ===\n",
    "    text_config = DecoderConfig()\n",
    "    vision_config = EncoderConfig()\n",
    "    text_config.vocab_size += 1\n",
    "    image_token_index = text_config.vocab_size - 1\n",
    "    config = PaliGemmaConfig(\n",
    "        text_config=text_config,\n",
    "        vision_config=vision_config,\n",
    "        image_token_index=image_token_index,\n",
    "    )\n",
    "\n",
    "    # === Load Model ===\n",
    "    model = PaliGemmaForConditionalGeneration(config).to(device)\n",
    "\n",
    "    # Tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "    tokenizer.add_tokens('<image>')\n",
    "    bert = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\").to(device)\n",
    "\n",
    "    # === Dataloader ===\n",
    "    train_loader = CustomDataLoader(\n",
    "        split=\"train\",\n",
    "        batch_size=batch_size,\n",
    "        num_workers=1,\n",
    "        tokenizer=tokenizer,\n",
    "        shuffle=False\n",
    "    )\n",
    "    val_loader = CustomDataLoader(\n",
    "        split=\"val\",\n",
    "        batch_size=batch_size,\n",
    "        num_workers=1,\n",
    "        tokenizer=tokenizer,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    # === Optimizer and Scheduler ===\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    num_training_steps = epochs * len(train_loader) // grad_accumulation_steps\n",
    "    # lr_scheduler = get_scheduler(\n",
    "    #     \"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    "    # )\n",
    "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, threshold=0.0001, threshold_mode='abs')\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "\n",
    "    # === Training Loop ===\n",
    "    start_time = time.time()\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        avg_train_loss = train(\n",
    "            model,\n",
    "            train_loader,\n",
    "            optimizer,\n",
    "            # lr_scheduler,\n",
    "            device,\n",
    "            epoch,\n",
    "            bert,\n",
    "            grad_accumulation_steps,\n",
    "            use_amp=use_amp,\n",
    "        )\n",
    "        avg_val_loss = validate(model, val_loader, device, bert, use_amp)\n",
    "        lr_scheduler.step(avg_val_loss)\n",
    "        train_loss.append(avg_train_loss)\n",
    "        val_loss.append(avg_val_loss)\n",
    "        print(f\"Epoch {epoch} - Train Loss: {avg_train_loss:.4f} - Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            print(\"Validation loss improved. Saving model...\")\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "            torch.save(model.state_dict(), f\"checkpoints/exp_1.pt\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"No improvement. Patience: {patience_counter}/{patience}\")\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "    save_dir = \"plot\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Plotting\n",
    "    epochs = list(range(1, len(train_loss) + 1))\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(epochs, train_loss, label='Train Loss', marker='o')\n",
    "    plt.plot(epochs, val_loss, label='Validation Loss', marker='x')\n",
    "    plt.title(\"Training and Validation Loss per Epoch\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Save the plot\n",
    "    plot_path = os.path.join(save_dir, \"exp_1.png\")\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()\n",
    "\n",
    "    # Calculate elapsed time\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Elapsed time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a817e6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Epoch 1:   0%|          | 0/33 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 1: 100%|██████████| 33/33 [00:35<00:00,  1.07s/it]\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Validating: 100%|██████████| 5/5 [00:05<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train Loss: 6.7990 - Val Loss: 4.1630\n",
      "Validation loss improved. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:   0%|          | 0/33 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 2: 100%|██████████| 33/33 [00:35<00:00,  1.07s/it]\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Validating: 100%|██████████| 5/5 [00:05<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Train Loss: 3.7991 - Val Loss: 3.3207\n",
      "Validation loss improved. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:   0%|          | 0/33 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 3: 100%|██████████| 33/33 [00:35<00:00,  1.08s/it]\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Validating: 100%|██████████| 5/5 [00:05<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Train Loss: 3.1113 - Val Loss: 2.9347\n",
      "Validation loss improved. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4:   0%|          | 0/33 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 4: 100%|██████████| 33/33 [00:35<00:00,  1.07s/it]\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Validating: 100%|██████████| 5/5 [00:05<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Train Loss: 2.8735 - Val Loss: 2.8092\n",
      "Validation loss improved. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5:   0%|          | 0/33 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 5: 100%|██████████| 33/33 [00:35<00:00,  1.08s/it]\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Validating: 100%|██████████| 5/5 [00:05<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Train Loss: 2.7840 - Val Loss: 2.7462\n",
      "Validation loss improved. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6:   0%|          | 0/33 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 6: 100%|██████████| 33/33 [00:35<00:00,  1.08s/it]\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Validating: 100%|██████████| 5/5 [00:05<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Train Loss: 2.7340 - Val Loss: 2.7029\n",
      "Validation loss improved. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:   0%|          | 0/33 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 7: 100%|██████████| 33/33 [00:35<00:00,  1.06s/it]\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Validating: 100%|██████████| 5/5 [00:05<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Train Loss: 2.6901 - Val Loss: 2.6641\n",
      "Validation loss improved. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8:   0%|          | 0/33 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 8: 100%|██████████| 33/33 [00:35<00:00,  1.07s/it]\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Validating: 100%|██████████| 5/5 [00:05<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Train Loss: 2.6586 - Val Loss: 2.6399\n",
      "Validation loss improved. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9:   0%|          | 0/33 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 9: 100%|██████████| 33/33 [00:35<00:00,  1.06s/it]\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Validating: 100%|██████████| 5/5 [00:05<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Train Loss: 2.6329 - Val Loss: 2.6174\n",
      "Validation loss improved. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10:   0%|          | 0/33 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 10: 100%|██████████| 33/33 [00:35<00:00,  1.07s/it]\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Validating: 100%|██████████| 5/5 [00:05<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Train Loss: 2.6089 - Val Loss: 2.5934\n",
      "Validation loss improved. Saving model...\n",
      "Elapsed time: 436.1700131893158 seconds\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87a85d1d-bcc9-4093-901b-02f080ddc7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import clip\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "from dataloaders_coba import CustomDataLoader\n",
    "from custom_tokenizers import Tokenizer\n",
    "from types import SimpleNamespace\n",
    "from decoder_coba import KVCache, PaliGemmaForConditionalGeneration\n",
    "from configs.config import DataConfig, EncoderConfig, DecoderConfig, PaliGemmaConfig\n",
    "from metrics import compute_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09f9d9d0-d452-4c26-8e63-daebc2a1f589",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    inputs_embeds,\n",
    "    pixel_values,\n",
    "    image_token_index,\n",
    "    max_length=60,\n",
    "    num_beams=5,\n",
    "    eos_token_id=2,\n",
    "    pad_token_id=0,\n",
    "    device=\"cuda\",\n",
    "):\n",
    "    model.eval()\n",
    "    batch_size = pixel_values.size(0)\n",
    "    eos_token_id = eos_token_id or tokenizer.eos_token_id\n",
    "\n",
    "    # Encode images\n",
    "    B, N, C, H, W = pixel_values.shape\n",
    "    pixel_values = pixel_values.view(B * N, C, H, W).to(device)\n",
    "    pixel_values = pixel_values.to(dtype=next(model.vision_tower.parameters()).dtype, device=device)\n",
    "    vision_features = model.vision_tower(pixel_values)\n",
    "    vision_features = vision_features.view(B, N, *vision_features.shape[1:])\n",
    "    image_features = torch.cat([vision_features[:, 0], vision_features[:, 1]], dim=1)\n",
    "    image_features = model.multi_modal_projector(image_features)  # shape: [B, Seq, Hidden]\n",
    "\n",
    "    # Init input_ids with <image> tokens\n",
    "    input_ids = torch.full((batch_size * num_beams, 1), image_token_index, dtype=torch.long, device=device)\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "\n",
    "    # Expand inputs for beams\n",
    "    image_features = image_features.unsqueeze(1).repeat(1, num_beams, 1, 1)\n",
    "    image_features = image_features.view(batch_size * num_beams, *image_features.shape[2:])\n",
    "\n",
    "    inputs_embeds = inputs_embeds.unsqueeze(1).repeat(1, num_beams, 1, 1)  # [B, num_beams, 1, 768]\n",
    "    inputs_embeds = inputs_embeds.view(batch_size * num_beams, 1, -1)      # [B*num_beams, 1, 768]\n",
    "\n",
    "\n",
    "    beam_scores = torch.zeros((batch_size, num_beams), device=device)\n",
    "    beam_scores[:, 1:] = -1e9\n",
    "    beam_scores = beam_scores.view(-1)\n",
    "\n",
    "    sequences = input_ids\n",
    "    is_done = [False] * batch_size\n",
    "\n",
    "    for step in range(max_length):\n",
    "\n",
    "        merged_input, attn_mask, pos_ids = model._merge_input_ids_with_image_features(\n",
    "            input_ids=sequences,\n",
    "            image_features=image_features,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=attention_mask,\n",
    "            kv_cache=None\n",
    "        )\n",
    "\n",
    "        outputs = model.language_model(\n",
    "            inputs_embeds=merged_input,\n",
    "            attention_mask=attn_mask,\n",
    "            position_ids=pos_ids,\n",
    "        )\n",
    "        logits = outputs[\"logits\"]\n",
    "        next_token_logits = logits[:, -1, :]\n",
    "        next_token_log_probs = F.log_softmax(next_token_logits, dim=-1)\n",
    "\n",
    "        next_token_log_probs = next_token_log_probs + beam_scores[:, None]\n",
    "\n",
    "        vocab_size = next_token_log_probs.size(-1)\n",
    "        next_token_log_probs = next_token_log_probs.view(batch_size, num_beams * vocab_size)\n",
    "        next_token_log_probs[:, image_token_index] = -1e9\n",
    "        topk_log_probs, topk_indices = torch.topk(next_token_log_probs, num_beams, dim=-1)\n",
    "\n",
    "        beam_indices = topk_indices // vocab_size\n",
    "        token_indices = topk_indices % vocab_size\n",
    "\n",
    "        sequences = sequences.view(batch_size, num_beams, -1)\n",
    "        new_sequences = [sequences[i, beam_indices[i]] for i in range(batch_size)]\n",
    "        sequences = torch.stack(new_sequences).view(batch_size * num_beams, -1)\n",
    "        sequences = torch.cat([sequences, token_indices.view(-1, 1)], dim=-1)\n",
    "\n",
    "        beam_scores = topk_log_probs.view(-1)\n",
    "        attention_mask = torch.cat([attention_mask, torch.ones_like(token_indices.view(-1, 1))], dim=1)\n",
    "\n",
    "        if eos_token_id is not None:\n",
    "            for i in range(batch_size):\n",
    "                done_for_beam = True\n",
    "                for beam_id in range(num_beams):\n",
    "                    token = sequences[i * num_beams + beam_id, -1]\n",
    "                    if token != eos_token_id:\n",
    "                        done_for_beam = False\n",
    "                        break\n",
    "                is_done[i] = done_for_beam\n",
    "            if all(is_done):\n",
    "                break\n",
    "\n",
    "    sequences = sequences.view(batch_size, num_beams, -1)\n",
    "    beam_scores = beam_scores.view(batch_size, num_beams)\n",
    "    best_indices = torch.argmax(beam_scores, dim=1)\n",
    "\n",
    "    best_sequences = [sequences[i, best_indices[i]] for i in range(batch_size)]\n",
    "    return torch.stack(best_sequences)\n",
    "\n",
    "def evaluate_model(model, tokenizer, dataloader, device, image_token_index, max_len=60, num_beams=5):\n",
    "    model.eval()\n",
    "    gts = {}\n",
    "    res = {}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tqdm(dataloader, desc=\"Evaluating\")):\n",
    "            # From your dataset: (image_id, image, ids, mask, bert, len(ids))\n",
    "            targets, pixel_values, input_ids, masks, inputs_embeds = batch\n",
    "\n",
    "            pixel_values = pixel_values.to(device)\n",
    "            inputs_embeds = inputs_embeds.unsqueeze(1).to(device)\n",
    "\n",
    "            generated_ids = beam_search_generate(\n",
    "                model,\n",
    "                tokenizer,\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                pixel_values=pixel_values,\n",
    "                image_token_index=image_token_index,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                device=device,\n",
    "                num_beams=num_beams,\n",
    "                max_length=max_len,\n",
    "            )\n",
    "\n",
    "            decoded_preds = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "            decoded_preds = [pred.strip().lower() for pred in decoded_preds]\n",
    "\n",
    "            references = targets\n",
    "            references = [tokenizer.decode(ref.tolist(), skip_special_tokens=True).replace(\"<image>\", \"\").strip().lower() for ref in references]\n",
    "\n",
    "\n",
    "            batch_size = pixel_values.size(0)\n",
    "            for j in range(batch_size):\n",
    "                img_id = f\"img_{i * batch_size + j}\"\n",
    "                gts[img_id] = [references[j]]\n",
    "                res[img_id] = [decoded_preds[j]]\n",
    "            for i in range(1):\n",
    "                # Optional print\n",
    "                print(f\"Pred: {decoded_preds[i]}\")\n",
    "                print(f\"Ref : {references[i]}\")\n",
    "                print()\n",
    "\n",
    "    scores = compute_scores(gts, res)\n",
    "    for metric, score in scores.items():\n",
    "        print(f\"{metric.upper()}: {score:.4f}\")\n",
    "\n",
    "    return scores, gts, res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55193c40-a4d9-41e5-b753-0fcc98af0f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Evaluating:   0%|          | 0/10 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Evaluating:  10%|█         | 1/10 [00:10<01:36, 10.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred: <image>pped thexxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx <image>\n",
      "Ref : no pneumothorax pleural effusion or airspace consolidation. heart size is upper limits of normal. pulmonary vasculature appear within normal limits. xxxx xxxx are intact.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  20%|██        | 2/10 [00:20<01:19,  9.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred: <image> hostile no no no. no. no..... no.. no. no........... no.. no..... no <image>x.. no... <image>x......... <image>\n",
      "Ref : heart size within normal limits. no focal alveolar consolidation no definite pleural effusion seen. no typical findings of pulmonary edema. no pneumothorax.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  30%|███       | 3/10 [00:29<01:07,  9.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred: <image>lak the the............................ <image> and.......................... <image>\n",
      "Ref : normal cardiac contour. clear hyperexpanded lungs bilaterally with no pneumothorax or pleural effusion.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  40%|████      | 4/10 [00:39<00:57,  9.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred: <image> score the arex....................................................... <image>\n",
      "Ref : lungs are clear bilaterally. there is no focal consolidation pleural effusion or pneumothoraces. cardiomediastinal silhouette is within normal limits. xxxx are unremarkable.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  50%|█████     | 5/10 [00:48<00:48,  9.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred: <image>lak the the............................x........................... <image>\n",
      "Ref : heart size within normal limits. small nodular opacity in the right upper lobe. this does not look like an acute infiltrate and more xxxx represents a granuloma. no pneumothorax or effusions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  60%|██████    | 6/10 [00:58<00:38,  9.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred: <image> hostile the the are are are are are......................................... the <image>x....... <image>\n",
      "Ref : the cardiomediastinal silhouette and pulmonary vasculature are within normal limits in size. the lungs are clear of focal airspace disease pneumothorax or pleural effusion. there are no acute bony findings.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  70%|███████   | 7/10 [01:08<00:29,  9.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred: <image> hostile the are are.. are are............... <image> and....................... <image>x <image>x....... <image>\n",
      "Ref : both lungs are clear and expanded. heart and mediastinum normal.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  80%|████████  | 8/10 [01:18<00:19,  9.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred: <image> exploits the the.............. <image> and........................................ <image>\n",
      "Ref : heart size and mediastinal contours appear within normal limits. pulmonary vascularity is within normal limits. no focal consolidation suspicious pulmonary opacity pneumothorax or definite pleural effusion. visualized osseous structures appear intact.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  90%|█████████ | 9/10 [01:28<00:09,  9.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred: <image> secondary <image> andststst.................................................\n",
      "Ref : lungs are hyperinflated but clear. no focal infiltrate or effusion. heart and mediastinal contours within normal limits. calcified mediastinal xxxx identified.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 10/10 [01:30<00:00,  9.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred: <image>lak thexxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx.xxxxxxxxxxxxxxxxxxxx <image>\n",
      "Ref : xxxx sternotomy xxxx and mediastinal postsurgical changes. stable cardiomegaly. crowded bronchovascular and interstitial markings xxxx related to low lung volumes and technique. grossly stable appearance of the lungs compared to prior exam without overt edema or gross airspace consolidation.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU_1: 0.0153\n",
      "BLEU_2: 0.0014\n",
      "BLEU_3: 0.0000\n",
      "BLEU_4: 0.0000\n",
      "METEOR: 0.0121\n",
      "ROUGE_L: 0.0526\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'BLEU_1': 0.015308923701470682,\n",
       " 'BLEU_2': 0.0013710466038697627,\n",
       " 'BLEU_3': 3.7567492191074755e-09,\n",
       " 'BLEU_4': 6.46292172358475e-12,\n",
       " 'METEOR': 0.012082561070748834,\n",
       " 'ROUGE_L': np.float64(0.052584822845214665)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = PaliGemmaConfig(text_config=DecoderConfig(), vision_config=EncoderConfig())\n",
    "model = PaliGemmaForConditionalGeneration(config)\n",
    "model.load_state_dict(torch.load(\"checkpoints/exp_1.pt\", map_location=\"cuda\"))\n",
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "tokenizer.add_tokens('<image>')\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_clip, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "model = model.to(device)\n",
    "\n",
    "test_loader = CustomDataLoader(\n",
    "    split=\"test\",\n",
    "    batch_size=64,\n",
    "    num_workers=1,\n",
    "    tokenizer=tokenizer,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "scores, gts, res = evaluate_model(model, tokenizer, test_loader, device, image_token_index=28996)\n",
    "scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
