{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53852407",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "754e3a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-01 20:15:15.224380: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-01 20:15:15.235849: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1751400915.248881    9571 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1751400915.252683    9571 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1751400915.263790    9571 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751400915.263806    9571 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751400915.263808    9571 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751400915.263810    9571 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-01 20:15:15.267599: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from transformers import get_scheduler\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from custom_tokenizers import Tokenizer\n",
    "from configs.config import DataConfig, EncoderConfig, DecoderConfig, PaliGemmaConfig\n",
    "from decoder_coba import KVCache, PaliGemmaForConditionalGeneration\n",
    "from dataloaders_coba import CustomDataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf9671bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model,\n",
    "    dataloader,\n",
    "    optimizer,\n",
    "    device,\n",
    "    epoch,\n",
    "    bert,\n",
    "    grad_accumulation_steps=1,\n",
    "    max_grad_norm=1.0,\n",
    "    use_amp=False,\n",
    "):\n",
    "    model.train()\n",
    "    kv_cache = KVCache()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "    config = DecoderConfig()\n",
    "\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for step, batch in enumerate(tqdm(dataloader, desc=f\"Epoch {epoch}\")):\n",
    "        input_ids, pixel_values, target, attention_mask, input_embeds = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        pixel_values = pixel_values.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        input_embeds = input_embeds.unsqueeze(1).to(device)\n",
    "        with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                pixel_values=pixel_values,\n",
    "                attention_mask=attention_mask,\n",
    "                inputs_embeds=input_embeds\n",
    "            )\n",
    "            \n",
    "            logits = outputs[\"logits\"]\n",
    "            shift_logits = logits[:, :-1, :].contiguous()\n",
    "            shift_labels = input_ids[:, 1:].contiguous()\n",
    "            \n",
    "            loss = torch.nn.functional.cross_entropy(\n",
    "                shift_logits.view(-1, config.vocab_size),\n",
    "                shift_labels.view(-1),\n",
    "                label_smoothing=0.1\n",
    "            )\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if (step + 1) % grad_accumulation_steps == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, dataloader, device, bert, use_amp=False):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "    config = DecoderConfig()\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Validating\"):\n",
    "        input_ids, pixel_values, target, attention_mask, input_embeds = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        pixel_values = pixel_values.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        input_embeds = input_embeds.unsqueeze(1).to(device)\n",
    "        with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                pixel_values=pixel_values,\n",
    "                attention_mask=attention_mask,\n",
    "                inputs_embeds=input_embeds\n",
    "            )\n",
    "            logits = outputs[\"logits\"]  # shape: [B, T, V]\n",
    "            shift_logits = logits[:, :-1, :].contiguous()\n",
    "            shift_labels = input_ids[:, 1:].contiguous()\n",
    "            \n",
    "            loss = torch.nn.functional.cross_entropy(\n",
    "                shift_logits.view(-1, config.vocab_size),\n",
    "                shift_labels.view(-1),\n",
    "                label_smoothing=0.1\n",
    "            )\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c993f54d-10a4-468c-965f-38083bd1666e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing embeddings for 2069 samples in split 'train'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2069 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "100%|██████████| 2069/2069 [00:07<00:00, 262.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved train embeddings to 'cached_embeds/train_bert_embeds.pt'\n",
      "Computing embeddings for 296 samples in split 'val'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/296 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "100%|██████████| 296/296 [00:01<00:00, 270.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved val embeddings to 'cached_embeds/val_bert_embeds.pt'\n",
      "Computing embeddings for 590 samples in split 'test'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/590 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "100%|██████████| 590/590 [00:02<00:00, 271.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved test embeddings to 'cached_embeds/test_bert_embeds.pt'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from configs.config import DataConfig\n",
    "from tqdm import tqdm\n",
    "\n",
    "def compute_and_save_bert_embeddings(split=\"train\", save_path=\"cached_bert_embeds.pt\"):\n",
    "    # === Load BERT model ===\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "    model = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\").eval().cuda()\n",
    "\n",
    "    # === Load annotations ===\n",
    "    config = DataConfig()\n",
    "    annotation_path = config.annotation_path\n",
    "    with open(annotation_path, 'r') as f:\n",
    "        data = json.load(f)[split]\n",
    "\n",
    "    print(f\"Computing embeddings for {len(data)} samples in split '{split}'...\")\n",
    "\n",
    "    embeddings = []\n",
    "    for example in tqdm(data):\n",
    "        report = example[\"report\"]\n",
    "\n",
    "        # Tokenize\n",
    "        inputs = tokenizer(report, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            last_hidden_state = outputs.last_hidden_state\n",
    "            attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "            # Average the hidden states (excluding padding)\n",
    "            masked = last_hidden_state * attention_mask.unsqueeze(-1)\n",
    "            mean_embedding = masked.sum(dim=1) / attention_mask.sum(dim=1, keepdim=True)\n",
    "\n",
    "        # Save as 1D CPU tensor (remove batch dimension)\n",
    "        embeddings.append(mean_embedding.squeeze(0).cpu())\n",
    "\n",
    "    # === Save all embeddings ===\n",
    "    os.makedirs(\"cached_embeds\", exist_ok=True)\n",
    "    torch.save(embeddings, os.path.join(\"cached_embeds\", f\"{split}_bert_embeds.pt\"))\n",
    "    print(f\"Saved {split} embeddings to 'cached_embeds/{split}_bert_embeds.pt'\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    compute_and_save_bert_embeddings(split=\"train\")\n",
    "    compute_and_save_bert_embeddings(split=\"val\")\n",
    "    compute_and_save_bert_embeddings(split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58d00b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Hyperparameters\n",
    "    epochs = 10\n",
    "    batch_size = 64\n",
    "    learning_rate = 3e-5\n",
    "    grad_accumulation_steps = 1\n",
    "    use_amp = True\n",
    "    patience = 30\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    # === Load Config ===\n",
    "    text_config = DecoderConfig()\n",
    "    vision_config = EncoderConfig()\n",
    "    text_config.vocab_size += 1\n",
    "    image_token_index = text_config.vocab_size - 1\n",
    "    config = PaliGemmaConfig(\n",
    "        text_config=text_config,\n",
    "        vision_config=vision_config,\n",
    "        image_token_index=image_token_index,\n",
    "    )\n",
    "\n",
    "    # === Load Model ===\n",
    "    model = PaliGemmaForConditionalGeneration(config).to(device)\n",
    "\n",
    "    # Tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "    tokenizer.add_tokens('<image>')\n",
    "    bert = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\").to(device)\n",
    "\n",
    "    # === Dataloader ===\n",
    "    train_loader = CustomDataLoader(\n",
    "        split=\"train\",\n",
    "        batch_size=batch_size,\n",
    "        num_workers=1,\n",
    "        tokenizer=tokenizer,\n",
    "        shuffle=False\n",
    "    )\n",
    "    val_loader = CustomDataLoader(\n",
    "        split=\"val\",\n",
    "        batch_size=batch_size,\n",
    "        num_workers=1,\n",
    "        tokenizer=tokenizer,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    # === Optimizer and Scheduler ===\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    num_training_steps = epochs * len(train_loader) // grad_accumulation_steps\n",
    "    # lr_scheduler = get_scheduler(\n",
    "    #     \"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    "    # )\n",
    "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, threshold=0.0001, threshold_mode='abs')\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "\n",
    "    # === Training Loop ===\n",
    "    start_time = time.time()\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        avg_train_loss = train(\n",
    "            model,\n",
    "            train_loader,\n",
    "            optimizer,\n",
    "            # lr_scheduler,\n",
    "            device,\n",
    "            epoch,\n",
    "            bert,\n",
    "            grad_accumulation_steps,\n",
    "            use_amp=use_amp,\n",
    "        )\n",
    "        avg_val_loss = validate(model, val_loader, device, bert, use_amp)\n",
    "        lr_scheduler.step(avg_val_loss)\n",
    "        train_loss.append(avg_train_loss)\n",
    "        val_loss.append(avg_val_loss)\n",
    "        print(f\"Epoch {epoch} - Train Loss: {avg_train_loss:.4f} - Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            print(\"Validation loss improved. Saving model...\")\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "            torch.save(model.state_dict(), f\"checkpoints/exp_1.pt\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"No improvement. Patience: {patience_counter}/{patience}\")\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "    save_dir = \"plot\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Plotting\n",
    "    epochs = list(range(1, len(train_loss) + 1))\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(epochs, train_loss, label='Train Loss', marker='o')\n",
    "    plt.plot(epochs, val_loss, label='Validation Loss', marker='x')\n",
    "    plt.title(\"Training and Validation Loss per Epoch\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Save the plot\n",
    "    plot_path = os.path.join(save_dir, \"exp_1.png\")\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()\n",
    "\n",
    "    # Calculate elapsed time\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Elapsed time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a817e6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Epoch 1:   0%|          | 0/33 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 1: 100%|██████████| 33/33 [00:35<00:00,  1.06s/it]\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Validating: 100%|██████████| 5/5 [00:05<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train Loss: 6.8171 - Val Loss: 4.2355\n",
      "Validation loss improved. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:   0%|          | 0/33 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 2: 100%|██████████| 33/33 [00:35<00:00,  1.07s/it]\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Validating: 100%|██████████| 5/5 [00:05<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Train Loss: 4.0094 - Val Loss: 3.6541\n",
      "Validation loss improved. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:   0%|          | 0/33 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 3: 100%|██████████| 33/33 [00:35<00:00,  1.07s/it]\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Validating: 100%|██████████| 5/5 [00:05<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Train Loss: 3.4693 - Val Loss: 3.2869\n",
      "Validation loss improved. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4:   0%|          | 0/33 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 4: 100%|██████████| 33/33 [00:34<00:00,  1.06s/it]\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Validating: 100%|██████████| 5/5 [00:05<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Train Loss: 3.2419 - Val Loss: 3.1784\n",
      "Validation loss improved. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5:   0%|          | 0/33 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 5: 100%|██████████| 33/33 [00:35<00:00,  1.06s/it]\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Validating: 100%|██████████| 5/5 [00:05<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Train Loss: 3.1857 - Val Loss: 3.1569\n",
      "Validation loss improved. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6:   0%|          | 0/33 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 6: 100%|██████████| 33/33 [00:34<00:00,  1.05s/it]\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Validating: 100%|██████████| 5/5 [00:05<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Train Loss: 3.1736 - Val Loss: 3.1498\n",
      "Validation loss improved. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:   0%|          | 0/33 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 7: 100%|██████████| 33/33 [00:35<00:00,  1.06s/it]\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Validating: 100%|██████████| 5/5 [00:05<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Train Loss: 3.1669 - Val Loss: 3.1423\n",
      "Validation loss improved. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8:   0%|          | 0/33 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 8: 100%|██████████| 33/33 [00:35<00:00,  1.07s/it]\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Validating: 100%|██████████| 5/5 [00:05<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Train Loss: 3.1594 - Val Loss: 3.1351\n",
      "Validation loss improved. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9:   0%|          | 0/33 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 9: 100%|██████████| 33/33 [00:34<00:00,  1.06s/it]\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Validating: 100%|██████████| 5/5 [00:05<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Train Loss: 3.1516 - Val Loss: 3.1256\n",
      "Validation loss improved. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10:   0%|          | 0/33 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 10: 100%|██████████| 33/33 [00:34<00:00,  1.06s/it]\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Validating: 100%|██████████| 5/5 [00:05<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Train Loss: 3.1427 - Val Loss: 3.1159\n",
      "Validation loss improved. Saving model...\n",
      "Elapsed time: 429.63511419296265 seconds\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87a85d1d-bcc9-4093-901b-02f080ddc7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import clip\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "from dataloaders_coba import CustomDataLoader\n",
    "from custom_tokenizers import Tokenizer\n",
    "from types import SimpleNamespace\n",
    "from decoder_coba import KVCache, PaliGemmaForConditionalGeneration\n",
    "from configs.config import DataConfig, EncoderConfig, DecoderConfig, PaliGemmaConfig\n",
    "from metrics import compute_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09f9d9d0-d452-4c26-8e63-daebc2a1f589",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    pixel_values,\n",
    "    image_token_index,\n",
    "    max_length=60,\n",
    "    num_beams=5,\n",
    "    eos_token_id=2,\n",
    "    pad_token_id=0,\n",
    "    device=\"cuda\",\n",
    "):\n",
    "    model.eval()\n",
    "    batch_size = pixel_values.size(0)\n",
    "    eos_token_id = eos_token_id or tokenizer.eos_token_id\n",
    "\n",
    "    # Encode images\n",
    "    B, N, C, H, W = pixel_values.shape\n",
    "    pixel_values = pixel_values.view(B * N, C, H, W).to(device)\n",
    "    pixel_values = pixel_values.to(dtype=next(model.vision_tower.parameters()).dtype, device=device)\n",
    "    vision_features = model.vision_tower(pixel_values)\n",
    "    vision_features = vision_features.view(B, N, *vision_features.shape[1:])\n",
    "    image_features = torch.cat([vision_features[:, 0], vision_features[:, 1]], dim=1)\n",
    "    image_features = model.multi_modal_projector(image_features)  # shape: [B, Seq, Hidden]\n",
    "\n",
    "    # Init input_ids with <image> tokens\n",
    "    input_ids = torch.full((batch_size * num_beams, 1), image_token_index, dtype=torch.long, device=device)\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "\n",
    "    # Expand inputs for beams\n",
    "    image_features = image_features.unsqueeze(1).repeat(1, num_beams, 1, 1)\n",
    "    image_features = image_features.view(batch_size * num_beams, *image_features.shape[2:])\n",
    "\n",
    "    beam_scores = torch.zeros((batch_size, num_beams), device=device)\n",
    "    beam_scores[:, 1:] = -1e9\n",
    "    beam_scores = beam_scores.view(-1)\n",
    "\n",
    "    sequences = input_ids\n",
    "    is_done = [False] * batch_size\n",
    "\n",
    "    for step in range(max_length):\n",
    "        input_embeds = model.language_model.get_input_embeddings()(sequences)\n",
    "\n",
    "        merged_input, attn_mask, pos_ids = model._merge_input_ids_with_image_features(\n",
    "            input_ids=sequences,\n",
    "            image_features=image_features,\n",
    "            inputs_embeds=input_embeds,\n",
    "            attention_mask=attention_mask,\n",
    "            kv_cache=None\n",
    "        )\n",
    "\n",
    "        outputs = model.language_model(\n",
    "            inputs_embeds=merged_input,\n",
    "            attention_mask=attn_mask,\n",
    "            position_ids=pos_ids,\n",
    "        )\n",
    "        logits = outputs[\"logits\"]\n",
    "        next_token_logits = logits[:, -1, :]\n",
    "        next_token_log_probs = F.log_softmax(next_token_logits, dim=-1)\n",
    "\n",
    "        next_token_log_probs = next_token_log_probs + beam_scores[:, None]\n",
    "\n",
    "        vocab_size = next_token_log_probs.size(-1)\n",
    "        next_token_log_probs = next_token_log_probs.view(batch_size, num_beams * vocab_size)\n",
    "        topk_log_probs, topk_indices = torch.topk(next_token_log_probs, num_beams, dim=-1)\n",
    "\n",
    "        beam_indices = topk_indices // vocab_size\n",
    "        token_indices = topk_indices % vocab_size\n",
    "\n",
    "        sequences = sequences.view(batch_size, num_beams, -1)\n",
    "        new_sequences = [sequences[i, beam_indices[i]] for i in range(batch_size)]\n",
    "        sequences = torch.stack(new_sequences).view(batch_size * num_beams, -1)\n",
    "        sequences = torch.cat([sequences, token_indices.view(-1, 1)], dim=-1)\n",
    "\n",
    "        beam_scores = topk_log_probs.view(-1)\n",
    "        attention_mask = torch.cat([attention_mask, torch.ones_like(token_indices.view(-1, 1))], dim=1)\n",
    "\n",
    "        if eos_token_id is not None:\n",
    "            for i in range(batch_size):\n",
    "                done_for_beam = True\n",
    "                for beam_id in range(num_beams):\n",
    "                    token = sequences[i * num_beams + beam_id, -1]\n",
    "                    if token != eos_token_id:\n",
    "                        done_for_beam = False\n",
    "                        break\n",
    "                is_done[i] = done_for_beam\n",
    "            if all(is_done):\n",
    "                break\n",
    "\n",
    "    sequences = sequences.view(batch_size, num_beams, -1)\n",
    "    beam_scores = beam_scores.view(batch_size, num_beams)\n",
    "    best_indices = torch.argmax(beam_scores, dim=1)\n",
    "\n",
    "    best_sequences = [sequences[i, best_indices[i]] for i in range(batch_size)]\n",
    "    return torch.stack(best_sequences)\n",
    "\n",
    "def evaluate_model(model, tokenizer, dataloader, device, image_token_index, max_len=60, num_beams=5):\n",
    "    model.eval()\n",
    "    gts = {}\n",
    "    res = {}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tqdm(dataloader, desc=\"Evaluating\")):\n",
    "            # From your dataset: (image_id, image, ids, mask, bert, len(ids))\n",
    "            image_ids, pixel_values, input_ids, masks, _ = batch\n",
    "\n",
    "            pixel_values = pixel_values.to(device)\n",
    "\n",
    "            generated_ids = beam_search_generate(\n",
    "                model,\n",
    "                tokenizer,\n",
    "                pixel_values=pixel_values,\n",
    "                image_token_index=image_token_index,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                device=device,\n",
    "                num_beams=num_beams,\n",
    "                max_length=max_len,\n",
    "            )\n",
    "\n",
    "            decoded_preds = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "            decoded_preds = [pred.strip().lower() for pred in decoded_preds]\n",
    "\n",
    "            references = input_ids  # this contains GT tokens\n",
    "            references = [tokenizer.decode(ref, skip_special_tokens=True).replace(\"<image>\", \"\").strip().lower() for ref in references]\n",
    "\n",
    "            batch_size = pixel_values.size(0)\n",
    "            for j in range(batch_size):\n",
    "                img_id = image_ids[j]\n",
    "                gts[img_id] = [references[j]]\n",
    "                res[img_id] = [decoded_preds[j]]\n",
    "\n",
    "                # Optional print\n",
    "                print(f\"Pred: {decoded_preds[j]}\")\n",
    "                print(f\"Ref : {references[j]}\")\n",
    "                print()\n",
    "\n",
    "    scores = compute_scores(gts, res)\n",
    "    for metric, score in scores.items():\n",
    "        print(f\"{metric.upper()}: {score:.4f}\")\n",
    "\n",
    "    return scores, gts, res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55193c40-a4d9-41e5-b753-0fcc98af0f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Evaluating:   0%|          | 0/10 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Evaluating:   0%|          | 0/10 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 21\u001b[0m\n\u001b[1;32m     11\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     13\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m CustomDataLoader(\n\u001b[1;32m     14\u001b[0m     split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     15\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     19\u001b[0m )\n\u001b[0;32m---> 21\u001b[0m scores, gts, res \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_token_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m28996\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m scores\n",
      "Cell \u001b[0;32mIn[12], line 104\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, tokenizer, dataloader, device, image_token_index, max_len, num_beams)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m    103\u001b[0m         \u001b[38;5;66;03m# From your dataset: (image_id, image, ids, mask, bert, len(ids))\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m         image_ids, pixel_values, input_ids, masks, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[1;32m    106\u001b[0m         pixel_values \u001b[38;5;241m=\u001b[39m pixel_values\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    108\u001b[0m         generated_ids \u001b[38;5;241m=\u001b[39m beam_search_generate(\n\u001b[1;32m    109\u001b[0m             model,\n\u001b[1;32m    110\u001b[0m             tokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    116\u001b[0m             max_length\u001b[38;5;241m=\u001b[39mmax_len,\n\u001b[1;32m    117\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 5)"
     ]
    }
   ],
   "source": [
    "config = PaliGemmaConfig(text_config=DecoderConfig(), vision_config=EncoderConfig())\n",
    "model = PaliGemmaForConditionalGeneration(config)\n",
    "model.load_state_dict(torch.load(\"checkpoints/exp_1.pt\", map_location=\"cuda\"))\n",
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "tokenizer.add_tokens('<image>')\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_clip, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "model = model.to(device)\n",
    "\n",
    "test_loader = CustomDataLoader(\n",
    "    split=\"test\",\n",
    "    batch_size=64,\n",
    "    num_workers=1,\n",
    "    tokenizer=tokenizer,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "scores, gts, res = evaluate_model(model, tokenizer, test_loader, device, image_token_index=28996)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac5ed0fa-f904-4e1d-8cc7-0795497f65a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/10 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Evaluating:   0%|          | 0/10 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(test_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[0;32m----> 2\u001b[0m     image_ids, pixel_values, input_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(tqdm(test_loader, desc=\"Evaluating\")):\n",
    "    image_ids, pixel_values, input_ids = zip(*batch)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
