{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53852407",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62b056cc-60b0-4f48-92f2-7e717fa0f026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting clip@ git+https://github.com/openai/CLIP.git@dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1 (from -r requirements.txt (line 18))\n",
      "  Cloning https://github.com/openai/CLIP.git (to revision dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1) to /tmp/pip-install-6wtv59y7/clip_df97596d93b242949454a234f5151ba5\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-install-6wtv59y7/clip_df97596d93b242949454a234f5151ba5\n",
      "  Running command git rev-parse -q --verify 'sha^dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1'\n",
      "  Running command git fetch -q https://github.com/openai/CLIP.git dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
      "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting absl-py==2.2.2 (from -r requirements.txt (line 1))\n",
      "  Downloading absl_py-2.2.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting accelerate==1.6.0 (from -r requirements.txt (line 2))\n",
      "  Downloading accelerate-1.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting aiohappyeyeballs==2.6.1 (from -r requirements.txt (line 3))\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiohttp==3.11.18 (from -r requirements.txt (line 4))\n",
      "  Downloading aiohttp-3.11.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting aiosignal==1.3.2 (from -r requirements.txt (line 5))\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting albucore==0.0.23 (from -r requirements.txt (line 6))\n",
      "  Downloading albucore-0.0.23-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting albumentations==2.0.2 (from -r requirements.txt (line 7))\n",
      "  Downloading albumentations-2.0.2-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting annotated-types==0.7.0 (from -r requirements.txt (line 8))\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting anyio==4.9.0 (from -r requirements.txt (line 9))\n",
      "  Downloading anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting asttokens==3.0.0 (from -r requirements.txt (line 10))\n",
      "  Downloading asttokens-3.0.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting astunparse==1.6.3 (from -r requirements.txt (line 11))\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting attrs==25.3.0 (from -r requirements.txt (line 12))\n",
      "  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting banks==2.1.2 (from -r requirements.txt (line 13))\n",
      "  Downloading banks-2.1.2-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting beautifulsoup4==4.13.4 (from -r requirements.txt (line 14))\n",
      "  Downloading beautifulsoup4-4.13.4-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting certifi==2025.1.31 (from -r requirements.txt (line 15))\n",
      "  Downloading certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting charset-normalizer==3.4.1 (from -r requirements.txt (line 16))\n",
      "  Downloading charset_normalizer-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
      "Collecting click==8.1.8 (from -r requirements.txt (line 17))\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting colorama==0.4.6 (from -r requirements.txt (line 19))\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Collecting comm==0.2.2 (from -r requirements.txt (line 20))\n",
      "  Downloading comm-0.2.2-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting contourpy==1.3.1 (from -r requirements.txt (line 21))\n",
      "  Downloading contourpy-1.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting cycler==0.12.1 (from -r requirements.txt (line 22))\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting dataclasses-json==0.6.7 (from -r requirements.txt (line 23))\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting debugpy==1.8.12 (from -r requirements.txt (line 24))\n",
      "  Downloading debugpy-1.8.12-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: decorator==5.1.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 25)) (5.1.1)\n",
      "Collecting Deprecated==1.2.18 (from -r requirements.txt (line 26))\n",
      "  Downloading Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting dirtyjson==1.0.8 (from -r requirements.txt (line 27))\n",
      "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting distro==1.9.0 (from -r requirements.txt (line 28))\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting executing==2.2.0 (from -r requirements.txt (line 29))\n",
      "  Downloading executing-2.2.0-py2.py3-none-any.whl.metadata (8.9 kB)\n",
      "Collecting filelock==3.17.0 (from -r requirements.txt (line 30))\n",
      "  Downloading filelock-3.17.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting filetype==1.2.0 (from -r requirements.txt (line 31))\n",
      "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting fire==0.7.0 (from -r requirements.txt (line 32))\n",
      "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting flatbuffers==25.2.10 (from -r requirements.txt (line 33))\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting fonttools==4.55.8 (from -r requirements.txt (line 34))\n",
      "  Downloading fonttools-4.55.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (101 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.2/101.2 kB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting frozenlist==1.6.0 (from -r requirements.txt (line 35))\n",
      "  Downloading frozenlist-1.6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
      "Collecting fsspec==2024.12.0 (from -r requirements.txt (line 36))\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting ftfy==6.3.1 (from -r requirements.txt (line 37))\n",
      "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting gast==0.6.0 (from -r requirements.txt (line 38))\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta==0.2.0 (from -r requirements.txt (line 39))\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting greenlet==3.2.1 (from -r requirements.txt (line 40))\n",
      "  Downloading greenlet-3.2.1-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting griffe==1.7.3 (from -r requirements.txt (line 41))\n",
      "  Downloading griffe-1.7.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting grpcio==1.71.0 (from -r requirements.txt (line 42))\n",
      "  Downloading grpcio-1.71.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting h11==0.16.0 (from -r requirements.txt (line 43))\n",
      "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting h2==4.2.0 (from -r requirements.txt (line 44))\n",
      "  Downloading h2-4.2.0-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting h5py==3.13.0 (from -r requirements.txt (line 45))\n",
      "  Downloading h5py-3.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting hpack==4.1.0 (from -r requirements.txt (line 46))\n",
      "  Downloading hpack-4.1.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting httpcore==1.0.9 (from -r requirements.txt (line 47))\n",
      "  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting httpx==0.28.1 (from -r requirements.txt (line 48))\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting huggingface-hub==0.28.1 (from -r requirements.txt (line 49))\n",
      "  Downloading huggingface_hub-0.28.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting hyperframe==6.1.0 (from -r requirements.txt (line 50))\n",
      "  Downloading hyperframe-6.1.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting idna==3.10 (from -r requirements.txt (line 51))\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting imageio==2.37.0 (from -r requirements.txt (line 52))\n",
      "  Downloading imageio-2.37.0-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting ipykernel==6.29.5 (from -r requirements.txt (line 53))\n",
      "  Downloading ipykernel-6.29.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting ipython==8.31.0 (from -r requirements.txt (line 54))\n",
      "  Downloading ipython-8.31.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting ipywidgets==8.1.7 (from -r requirements.txt (line 55))\n",
      "  Downloading ipywidgets-8.1.7-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting jedi==0.19.2 (from -r requirements.txt (line 56))\n",
      "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting Jinja2==3.1.5 (from -r requirements.txt (line 57))\n",
      "  Downloading jinja2-3.1.5-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting jiter==0.9.0 (from -r requirements.txt (line 58))\n",
      "  Downloading jiter-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting joblib==1.5.0 (from -r requirements.txt (line 59))\n",
      "  Downloading joblib-1.5.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting jsonpatch==1.33 (from -r requirements.txt (line 60))\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting jsonpointer==3.0.0 (from -r requirements.txt (line 61))\n",
      "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting jupyter_client==8.6.3 (from -r requirements.txt (line 62))\n",
      "  Downloading jupyter_client-8.6.3-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting jupyter_core==5.7.2 (from -r requirements.txt (line 63))\n",
      "  Downloading jupyter_core-5.7.2-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting jupyterlab_widgets==3.0.15 (from -r requirements.txt (line 64))\n",
      "  Downloading jupyterlab_widgets-3.0.15-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting keras==3.9.2 (from -r requirements.txt (line 65))\n",
      "  Downloading keras-3.9.2-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting kiwisolver==1.4.8 (from -r requirements.txt (line 66))\n",
      "  Downloading kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.2 kB)\n",
      "Collecting langchain-core==0.3.58 (from -r requirements.txt (line 67))\n",
      "  Downloading langchain_core-0.3.58-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting langchain-ollama==0.3.2 (from -r requirements.txt (line 68))\n",
      "  Downloading langchain_ollama-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting langsmith==0.3.42 (from -r requirements.txt (line 69))\n",
      "  Downloading langsmith-0.3.42-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting lazy_loader==0.4 (from -r requirements.txt (line 70))\n",
      "  Downloading lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting libclang==18.1.1 (from -r requirements.txt (line 71))\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting llama-cloud==0.1.19 (from -r requirements.txt (line 72))\n",
      "  Downloading llama_cloud-0.1.19-py3-none-any.whl.metadata (902 bytes)\n",
      "Collecting llama-cloud-services==0.6.21 (from -r requirements.txt (line 73))\n",
      "  Downloading llama_cloud_services-0.6.21-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting llama-index==0.12.34 (from -r requirements.txt (line 74))\n",
      "  Downloading llama_index-0.12.34-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting llama-index-agent-openai==0.4.7 (from -r requirements.txt (line 75))\n",
      "  Downloading llama_index_agent_openai-0.4.7-py3-none-any.whl.metadata (438 bytes)\n",
      "Collecting llama-index-cli==0.4.1 (from -r requirements.txt (line 76))\n",
      "  Downloading llama_index_cli-0.4.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting llama-index-core==0.12.34.post1 (from -r requirements.txt (line 77))\n",
      "  Downloading llama_index_core-0.12.34.post1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting llama-index-embeddings-openai==0.3.1 (from -r requirements.txt (line 78))\n",
      "  Downloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl.metadata (684 bytes)\n",
      "Collecting llama-index-indices-managed-llama-cloud==0.6.11 (from -r requirements.txt (line 79))\n",
      "  Downloading llama_index_indices_managed_llama_cloud-0.6.11-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting llama-index-llms-ollama==0.5.4 (from -r requirements.txt (line 80))\n",
      "  Downloading llama_index_llms_ollama-0.5.4-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting llama-index-llms-openai==0.3.38 (from -r requirements.txt (line 81))\n",
      "  Downloading llama_index_llms_openai-0.3.38-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-index-multi-modal-llms-openai==0.4.3 (from -r requirements.txt (line 82))\n",
      "  Downloading llama_index_multi_modal_llms_openai-0.4.3-py3-none-any.whl.metadata (726 bytes)\n",
      "Collecting llama-index-program-openai==0.3.1 (from -r requirements.txt (line 83))\n",
      "  Downloading llama_index_program_openai-0.3.1-py3-none-any.whl.metadata (764 bytes)\n",
      "Collecting llama-index-question-gen-openai==0.3.0 (from -r requirements.txt (line 84))\n",
      "  Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl.metadata (783 bytes)\n",
      "Collecting llama-index-readers-file==0.4.7 (from -r requirements.txt (line 85))\n",
      "  Downloading llama_index_readers_file-0.4.7-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting llama-index-readers-llama-parse==0.4.0 (from -r requirements.txt (line 86))\n",
      "  Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting llama-parse==0.6.21 (from -r requirements.txt (line 87))\n",
      "  Downloading llama_parse-0.6.21-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting loralib==0.1.2 (from -r requirements.txt (line 88))\n",
      "  Downloading loralib-0.1.2-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting Markdown==3.8 (from -r requirements.txt (line 89))\n",
      "  Downloading markdown-3.8-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting markdown-it-py==3.0.0 (from -r requirements.txt (line 90))\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting MarkupSafe==3.0.2 (from -r requirements.txt (line 91))\n",
      "  Downloading MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting marshmallow==3.26.1 (from -r requirements.txt (line 92))\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting matplotlib==3.10.0 (from -r requirements.txt (line 93))\n",
      "  Downloading matplotlib-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting matplotlib-inline==0.1.7 (from -r requirements.txt (line 94))\n",
      "  Downloading matplotlib_inline-0.1.7-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting mdurl==0.1.2 (from -r requirements.txt (line 95))\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting ml_dtypes==0.5.1 (from -r requirements.txt (line 96))\n",
      "  Downloading ml_dtypes-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
      "Requirement already satisfied: mpmath==1.3.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 97)) (1.3.0)\n",
      "Collecting multidict==6.4.3 (from -r requirements.txt (line 98))\n",
      "  Downloading multidict-6.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting mypy_extensions==1.1.0 (from -r requirements.txt (line 99))\n",
      "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting namex==0.0.9 (from -r requirements.txt (line 100))\n",
      "  Downloading namex-0.0.9-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting nest-asyncio==1.6.0 (from -r requirements.txt (line 101))\n",
      "  Downloading nest_asyncio-1.6.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting networkx==3.4.2 (from -r requirements.txt (line 102))\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting nltk==3.9.1 (from -r requirements.txt (line 103))\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 104)) (1.24.1)\n",
      "Collecting ollama==0.4.8 (from -r requirements.txt (line 105))\n",
      "  Downloading ollama-0.4.8-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting openai==1.77.0 (from -r requirements.txt (line 106))\n",
      "  Downloading openai-1.77.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting opencv-python-headless (from -r requirements.txt (line 107))\n",
      "  Downloading opencv_python_headless-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (19 kB)\n",
      "Collecting opt_einsum==3.4.0 (from -r requirements.txt (line 108))\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting optree==0.15.0 (from -r requirements.txt (line 109))\n",
      "  Downloading optree-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting orjson==3.10.18 (from -r requirements.txt (line 110))\n",
      "  Downloading orjson-3.10.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting packaging==24.2 (from -r requirements.txt (line 111))\n",
      "  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting pandas==2.2.3 (from -r requirements.txt (line 112))\n",
      "  Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting parso==0.8.4 (from -r requirements.txt (line 113))\n",
      "  Downloading parso-0.8.4-py2.py3-none-any.whl.metadata (7.7 kB)\n",
      "Collecting peft==0.15.2 (from -r requirements.txt (line 114))\n",
      "  Downloading peft-0.15.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pillow==11.1.0 (from -r requirements.txt (line 115))\n",
      "  Downloading pillow-11.1.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
      "Collecting platformdirs==4.3.7 (from -r requirements.txt (line 116))\n",
      "  Downloading platformdirs-4.3.7-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting portalocker==2.10.1 (from -r requirements.txt (line 117))\n",
      "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting prompt_toolkit==3.0.50 (from -r requirements.txt (line 118))\n",
      "  Downloading prompt_toolkit-3.0.50-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting propcache==0.3.1 (from -r requirements.txt (line 119))\n",
      "  Downloading propcache-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting protobuf==5.29.4 (from -r requirements.txt (line 120))\n",
      "  Downloading protobuf-5.29.4-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Collecting psutil==6.1.1 (from -r requirements.txt (line 121))\n",
      "  Downloading psutil-6.1.1-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
      "Collecting pure_eval==0.2.3 (from -r requirements.txt (line 122))\n",
      "  Downloading pure_eval-0.2.3-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting pycocoevalcap==1.2 (from -r requirements.txt (line 123))\n",
      "  Downloading pycocoevalcap-1.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting pycocotools==2.0.9 (from -r requirements.txt (line 124))\n",
      "  Downloading pycocotools-2.0.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting pydantic==2.10.6 (from -r requirements.txt (line 125))\n",
      "  Downloading pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting pydantic_core==2.27.2 (from -r requirements.txt (line 126))\n",
      "  Downloading pydantic_core-2.27.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting Pygments==2.19.1 (from -r requirements.txt (line 127))\n",
      "  Downloading pygments-2.19.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting pyparsing==3.2.1 (from -r requirements.txt (line 128))\n",
      "  Downloading pyparsing-3.2.1-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting pypdf==5.4.0 (from -r requirements.txt (line 129))\n",
      "  Downloading pypdf-5.4.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting python-dateutil==2.9.0.post0 (from -r requirements.txt (line 130))\n",
      "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting python-dotenv==1.1.0 (from -r requirements.txt (line 131))\n",
      "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting pytz==2024.2 (from -r requirements.txt (line 132))\n",
      "  Downloading pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting PyYAML==6.0.2 (from -r requirements.txt (line 133))\n",
      "  Downloading PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting pyzmq==26.2.1 (from -r requirements.txt (line 134))\n",
      "  Downloading pyzmq-26.2.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (6.2 kB)\n",
      "Collecting qdrant-client==1.14.2 (from -r requirements.txt (line 135))\n",
      "  Downloading qdrant_client-1.14.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting regex==2024.11.6 (from -r requirements.txt (line 136))\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting requests==2.32.3 (from -r requirements.txt (line 137))\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting requests-toolbelt==1.0.0 (from -r requirements.txt (line 138))\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting rich==14.0.0 (from -r requirements.txt (line 139))\n",
      "  Downloading rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting safetensors==0.5.2 (from -r requirements.txt (line 140))\n",
      "  Downloading safetensors-0.5.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting scikit-image==0.25.2 (from -r requirements.txt (line 141))\n",
      "  Downloading scikit_image-0.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Collecting scipy==1.15.1 (from -r requirements.txt (line 142))\n",
      "  Downloading scipy-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting simsimd==6.2.1 (from -r requirements.txt (line 143))\n",
      "  Downloading simsimd-6.2.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (66 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.0/66.0 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting six==1.17.0 (from -r requirements.txt (line 144))\n",
      "  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting sniffio==1.3.1 (from -r requirements.txt (line 145))\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting soupsieve==2.7 (from -r requirements.txt (line 146))\n",
      "  Downloading soupsieve-2.7-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting SQLAlchemy==2.0.40 (from -r requirements.txt (line 147))\n",
      "  Downloading sqlalchemy-2.0.40-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: stack-data==0.6.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 148)) (0.6.3)\n",
      "Collecting stringzilla==3.11.3 (from -r requirements.txt (line 149))\n",
      "  Downloading stringzilla-3.11.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl.metadata (80 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.1/80.1 kB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting striprtf==0.0.26 (from -r requirements.txt (line 150))\n",
      "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting sympy==1.13.1 (from -r requirements.txt (line 151))\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting tenacity==9.1.2 (from -r requirements.txt (line 152))\n",
      "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting tensorboard==2.19.0 (from -r requirements.txt (line 153))\n",
      "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting tensorboard-data-server==0.7.2 (from -r requirements.txt (line 154))\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting tensorflow==2.19.0 (from -r requirements.txt (line 155))\n",
      "  Downloading tensorflow-2.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem==0.31.0 (from -r requirements.txt (line 156))\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (14 kB)\n",
      "Collecting termcolor==3.1.0 (from -r requirements.txt (line 157))\n",
      "  Downloading termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting tiktoken==0.9.0 (from -r requirements.txt (line 158))\n",
      "  Downloading tiktoken-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting timm==1.0.14 (from -r requirements.txt (line 159))\n",
      "  Downloading timm-1.0.14-py3-none-any.whl.metadata (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers==0.21.0 (from -r requirements.txt (line 160))\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting torch==2.6.0 (from -r requirements.txt (line 161))\n",
      "  Downloading torch-2.6.0-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Collecting torchvision==0.21.0 (from -r requirements.txt (line 162))\n",
      "  Downloading torchvision-0.21.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting tornado==6.4.2 (from -r requirements.txt (line 163))\n",
      "  Downloading tornado-6.4.2-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting tqdm==4.67.1 (from -r requirements.txt (line 164))\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting traitlets==5.14.3 (from -r requirements.txt (line 165))\n",
      "  Downloading traitlets-5.14.3-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting transformers==4.48.2 (from -r requirements.txt (line 166))\n",
      "  Downloading transformers-4.48.2-py3-none-any.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting typing-inspect==0.9.0 (from -r requirements.txt (line 167))\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting typing_extensions==4.12.2 (from -r requirements.txt (line 168))\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting tzdata==2024.2 (from -r requirements.txt (line 169))\n",
      "  Downloading tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting urllib3==2.3.0 (from -r requirements.txt (line 170))\n",
      "  Downloading urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting wcwidth==0.2.13 (from -r requirements.txt (line 171))\n",
      "  Downloading wcwidth-0.2.13-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting Werkzeug==3.1.3 (from -r requirements.txt (line 172))\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting widgetsnbextension==4.0.14 (from -r requirements.txt (line 173))\n",
      "  Downloading widgetsnbextension-4.0.14-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting wrapt==1.17.2 (from -r requirements.txt (line 174))\n",
      "  Downloading wrapt-1.17.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting yarl==1.20.0 (from -r requirements.txt (line 175))\n",
      "  Downloading yarl-1.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (72 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.4/72.4 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting zstandard==0.23.0 (from -r requirements.txt (line 176))\n",
      "  Downloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp==3.11.18->-r requirements.txt (line 4))\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting numpy (from -r requirements.txt (line 104))\n",
      "  Downloading numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio==4.9.0->-r requirements.txt (line 9)) (1.1.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse==1.6.3->-r requirements.txt (line 11)) (0.41.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython==8.31.0->-r requirements.txt (line 54)) (4.8.0)\n",
      "Collecting tifffile>=2022.8.12 (from scikit-image==0.25.2->-r requirements.txt (line 141))\n",
      "  Downloading tifffile-2025.5.10-py3-none-any.whl.metadata (31 kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.19.0->-r requirements.txt (line 153)) (68.2.2)\n",
      "Collecting numpy (from -r requirements.txt (line 104))\n",
      "  Downloading numpy-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.6.0->-r requirements.txt (line 161))\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.6.0->-r requirements.txt (line 161))\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.6.0->-r requirements.txt (line 161))\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.6.0->-r requirements.txt (line 161))\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.6.0->-r requirements.txt (line 161))\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.6.0->-r requirements.txt (line 161))\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.6.0->-r requirements.txt (line 161))\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.6.0->-r requirements.txt (line 161))\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.6.0->-r requirements.txt (line 161))\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch==2.6.0->-r requirements.txt (line 161))\n",
      "  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch==2.6.0->-r requirements.txt (line 161))\n",
      "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch==2.6.0->-r requirements.txt (line 161))\n",
      "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.6.0->-r requirements.txt (line 161))\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.2.0 (from torch==2.6.0->-r requirements.txt (line 161))\n",
      "  Downloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython==8.31.0->-r requirements.txt (line 54)) (0.7.0)\n",
      "Downloading absl_py-2.2.2-py3-none-any.whl (135 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.6.0-py3-none-any.whl (354 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m354.7/354.7 kB\u001b[0m \u001b[31m134.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiohttp-3.11.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m229.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading albucore-0.0.23-py3-none-any.whl (14 kB)\n",
      "Downloading albumentations-2.0.2-py3-none-any.whl (278 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m123.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.9/100.9 kB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading asttokens-3.0.0-py3-none-any.whl (26 kB)\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 kB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading banks-2.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m187.3/187.3 kB\u001b[0m \u001b[31m97.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m96.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading charset_normalizer-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (146 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.1/146.1 kB\u001b[0m \u001b[31m75.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Downloading comm-0.2.2-py3-none-any.whl (7.2 kB)\n",
      "Downloading contourpy-1.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (324 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.0/325.0 kB\u001b[0m \u001b[31m125.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading debugpy-1.8.12-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
      "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading executing-2.2.0-py2.py3-none-any.whl (26 kB)\n",
      "Downloading filelock-3.17.0-py3-none-any.whl (16 kB)\n",
      "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Downloading fonttools-4.55.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading frozenlist-1.6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (287 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.3/287.3 kB\u001b[0m \u001b[31m124.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m102.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading greenlet-3.2.1-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (580 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m580.6/580.6 kB\u001b[0m \u001b[31m182.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading griffe-1.7.3-py3-none-any.whl (129 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.3/129.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.71.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/5.9 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0mmm\n",
      "\u001b[?25h\u001b[31mERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/urllib3/response.py\", line 438, in _error_catcher\n",
      "    yield\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/urllib3/response.py\", line 561, in read\n",
      "    data = self._fp_read(amt) if not fp_closed else b\"\"\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/urllib3/response.py\", line 527, in _fp_read\n",
      "    return self._fp.read(amt) if amt is not None else self._fp.read()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/cachecontrol/filewrapper.py\", line 102, in read\n",
      "    self.__buf.write(data)\n",
      "  File \"/usr/lib/python3.10/tempfile.py\", line 622, in func_wrapper\n",
      "    return func(*args, **kwargs)\n",
      "OSError: [Errno 12] Cannot allocate memory\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 180, in exc_logging_wrapper\n",
      "    status = run_func(*args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/req_command.py\", line 245, in wrapper\n",
      "    return func(self, options, args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 377, in run\n",
      "    requirement_set = resolver.resolve(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 179, in resolve\n",
      "    self.factory.preparer.prepare_linked_requirements_more(reqs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/prepare.py\", line 552, in prepare_linked_requirements_more\n",
      "    self._complete_partial_requirements(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/prepare.py\", line 467, in _complete_partial_requirements\n",
      "    for link, (filepath, _) in batch_download:\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/network/download.py\", line 183, in __call__\n",
      "    for chunk in chunks:\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/progress_bars.py\", line 53, in _rich_progress_bar\n",
      "    for chunk in iterable:\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/network/utils.py\", line 63, in response_chunks\n",
      "    for chunk in response.raw.stream(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/urllib3/response.py\", line 622, in stream\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/urllib3/response.py\", line 560, in read\n",
      "    with self._error_catcher():\n",
      "  File \"/usr/lib/python3.10/contextlib.py\", line 153, in __exit__\n",
      "    self.gen.throw(typ, value, traceback)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/urllib3/response.py\", line 455, in _error_catcher\n",
      "    raise ProtocolError(\"Connection broken: %r\" % e, e)\n",
      "pip._vendor.urllib3.exceptions.ProtocolError: (\"Connection broken: OSError(12, 'Cannot allocate memory')\", OSError(12, 'Cannot allocate memory'))\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eff8bae3-fede-445e-9150-ab7aa0fa1f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get:1 http://archive.ubuntu.com/ubuntu jammy InRelease [270 kB]\n",
      "Get:2 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]      \n",
      "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1581 B]\n",
      "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1804 kB]\n",
      "Get:5 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]        \n",
      "Get:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
      "Get:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [33.2 kB]\n",
      "Get:9 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3098 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu jammy/universe amd64 Packages [17.5 MB]\n",
      "Get:11 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [48.5 kB]\n",
      "Get:12 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [4780 kB]\n",
      "Get:13 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1264 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu jammy/main amd64 Packages [1792 kB]    \n",
      "Get:15 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 Packages [266 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu jammy/restricted amd64 Packages [164 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1567 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [56.4 kB]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4948 kB]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3413 kB]\n",
      "Get:21 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [38.4 kB]\n",
      "Get:22 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [113 kB]\n",
      "Fetched 41.5 MB in 6s (7430 kB/s)                           \n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  alsa-topology-conf alsa-ucm-conf ca-certificates-java java-common libasound2\n",
      "  libasound2-data libavahi-client3 libavahi-common-data libavahi-common3\n",
      "  libcups2 libgraphite2-3 libharfbuzz0b liblcms2-2 libnspr4 libnss3\n",
      "  libpcsclite1\n",
      "Suggested packages:\n",
      "  default-jre libasound2-plugins alsa-utils cups-common liblcms2-utils pcscd\n",
      "  libnss-mdns fonts-dejavu-extra fonts-ipafont-gothic fonts-ipafont-mincho\n",
      "  fonts-wqy-microhei | fonts-wqy-zenhei fonts-indic\n",
      "The following NEW packages will be installed:\n",
      "  alsa-topology-conf alsa-ucm-conf ca-certificates-java java-common libasound2\n",
      "  libasound2-data libavahi-client3 libavahi-common-data libavahi-common3\n",
      "  libcups2 libgraphite2-3 libharfbuzz0b liblcms2-2 libnspr4 libnss3\n",
      "  libpcsclite1 openjdk-17-jre-headless\n",
      "0 upgraded, 17 newly installed, 0 to remove and 143 not upgraded.\n",
      "Need to get 51.2 MB of archives.\n",
      "After this operation, 203 MB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 alsa-topology-conf all 1.2.5.1-2 [15.5 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libasound2-data all 1.2.6.1-1ubuntu1 [19.1 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libasound2 amd64 1.2.6.1-1ubuntu1 [390 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 alsa-ucm-conf all 1.2.6.3-1ubuntu1.12 [43.5 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 java-common all 0.72build2 [6782 B]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libavahi-common-data amd64 0.8-5ubuntu5.2 [23.8 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libavahi-common3 amd64 0.8-5ubuntu5.2 [23.9 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libavahi-client3 amd64 0.8-5ubuntu5.2 [28.0 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libcups2 amd64 2.4.1op1-1ubuntu4.11 [263 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu jammy/main amd64 liblcms2-2 amd64 2.12~rc1-2build2 [159 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libnspr4 amd64 2:4.35-0ubuntu0.22.04.1 [119 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libnss3 amd64 2:3.98-0ubuntu0.22.04.2 [1347 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgraphite2-3 amd64 1.3.14-1build2 [71.3 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libharfbuzz0b amd64 2.7.4-1ubuntu3.2 [353 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpcsclite1 amd64 1.9.5-3ubuntu1 [19.8 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 openjdk-17-jre-headless amd64 17.0.15+6~us1-0ubuntu1~22.04 [48.3 MB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 ca-certificates-java all 20190909ubuntu1.2 [12.1 kB]\n",
      "Fetched 51.2 MB in 12s (4367 kB/s)                                             \n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "Selecting previously unselected package alsa-topology-conf.\n",
      "(Reading database ... 20729 files and directories currently installed.)\n",
      "Preparing to unpack .../00-alsa-topology-conf_1.2.5.1-2_all.deb ...\n",
      "Unpacking alsa-topology-conf (1.2.5.1-2) ...\n",
      "Selecting previously unselected package libasound2-data.\n",
      "Preparing to unpack .../01-libasound2-data_1.2.6.1-1ubuntu1_all.deb ...\n",
      "Unpacking libasound2-data (1.2.6.1-1ubuntu1) ...\n",
      "Selecting previously unselected package libasound2:amd64.\n",
      "Preparing to unpack .../02-libasound2_1.2.6.1-1ubuntu1_amd64.deb ...\n",
      "Unpacking libasound2:amd64 (1.2.6.1-1ubuntu1) ...\n",
      "Selecting previously unselected package alsa-ucm-conf.\n",
      "Preparing to unpack .../03-alsa-ucm-conf_1.2.6.3-1ubuntu1.12_all.deb ...\n",
      "Unpacking alsa-ucm-conf (1.2.6.3-1ubuntu1.12) ...\n",
      "Selecting previously unselected package java-common.\n",
      "Preparing to unpack .../04-java-common_0.72build2_all.deb ...\n",
      "Unpacking java-common (0.72build2) ...\n",
      "Selecting previously unselected package libavahi-common-data:amd64.\n",
      "Preparing to unpack .../05-libavahi-common-data_0.8-5ubuntu5.2_amd64.deb ...\n",
      "Unpacking libavahi-common-data:amd64 (0.8-5ubuntu5.2) ...\n",
      "Selecting previously unselected package libavahi-common3:amd64.\n",
      "Preparing to unpack .../06-libavahi-common3_0.8-5ubuntu5.2_amd64.deb ...\n",
      "Unpacking libavahi-common3:amd64 (0.8-5ubuntu5.2) ...\n",
      "Selecting previously unselected package libavahi-client3:amd64.\n",
      "Preparing to unpack .../07-libavahi-client3_0.8-5ubuntu5.2_amd64.deb ...\n",
      "Unpacking libavahi-client3:amd64 (0.8-5ubuntu5.2) ...\n",
      "Selecting previously unselected package libcups2:amd64.\n",
      "Preparing to unpack .../08-libcups2_2.4.1op1-1ubuntu4.11_amd64.deb ...\n",
      "Unpacking libcups2:amd64 (2.4.1op1-1ubuntu4.11) ...\n",
      "Selecting previously unselected package liblcms2-2:amd64.\n",
      "Preparing to unpack .../09-liblcms2-2_2.12~rc1-2build2_amd64.deb ...\n",
      "Unpacking liblcms2-2:amd64 (2.12~rc1-2build2) ...\n",
      "Selecting previously unselected package libnspr4:amd64.\n",
      "Preparing to unpack .../10-libnspr4_2%3a4.35-0ubuntu0.22.04.1_amd64.deb ...\n",
      "Unpacking libnspr4:amd64 (2:4.35-0ubuntu0.22.04.1) ...\n",
      "Selecting previously unselected package libnss3:amd64.\n",
      "Preparing to unpack .../11-libnss3_2%3a3.98-0ubuntu0.22.04.2_amd64.deb ...\n",
      "Unpacking libnss3:amd64 (2:3.98-0ubuntu0.22.04.2) ...\n",
      "Selecting previously unselected package libgraphite2-3:amd64.\n",
      "Preparing to unpack .../12-libgraphite2-3_1.3.14-1build2_amd64.deb ...\n",
      "Unpacking libgraphite2-3:amd64 (1.3.14-1build2) ...\n",
      "Selecting previously unselected package libharfbuzz0b:amd64.\n",
      "Preparing to unpack .../13-libharfbuzz0b_2.7.4-1ubuntu3.2_amd64.deb ...\n",
      "Unpacking libharfbuzz0b:amd64 (2.7.4-1ubuntu3.2) ...\n",
      "Selecting previously unselected package libpcsclite1:amd64.\n",
      "Preparing to unpack .../14-libpcsclite1_1.9.5-3ubuntu1_amd64.deb ...\n",
      "Unpacking libpcsclite1:amd64 (1.9.5-3ubuntu1) ...\n",
      "Selecting previously unselected package openjdk-17-jre-headless:amd64.\n",
      "Preparing to unpack .../15-openjdk-17-jre-headless_17.0.15+6~us1-0ubuntu1~22.04_amd64.deb ...\n",
      "Unpacking openjdk-17-jre-headless:amd64 (17.0.15+6~us1-0ubuntu1~22.04) ...\n",
      "Selecting previously unselected package ca-certificates-java.\n",
      "Preparing to unpack .../16-ca-certificates-java_20190909ubuntu1.2_all.deb ...\n",
      "Unpacking ca-certificates-java (20190909ubuntu1.2) ...\n",
      "Setting up libgraphite2-3:amd64 (1.3.14-1build2) ...\n",
      "Setting up liblcms2-2:amd64 (2.12~rc1-2build2) ...\n",
      "Setting up java-common (0.72build2) ...\n",
      "Setting up libasound2-data (1.2.6.1-1ubuntu1) ...\n",
      "Setting up libnspr4:amd64 (2:4.35-0ubuntu0.22.04.1) ...\n",
      "Setting up libavahi-common-data:amd64 (0.8-5ubuntu5.2) ...\n",
      "Setting up libpcsclite1:amd64 (1.9.5-3ubuntu1) ...\n",
      "Setting up alsa-topology-conf (1.2.5.1-2) ...\n",
      "Setting up libasound2:amd64 (1.2.6.1-1ubuntu1) ...\n",
      "Setting up libharfbuzz0b:amd64 (2.7.4-1ubuntu3.2) ...\n",
      "Setting up alsa-ucm-conf (1.2.6.3-1ubuntu1.12) ...\n",
      "Setting up libavahi-common3:amd64 (0.8-5ubuntu5.2) ...\n",
      "Setting up libnss3:amd64 (2:3.98-0ubuntu0.22.04.2) ...\n",
      "Setting up libavahi-client3:amd64 (0.8-5ubuntu5.2) ...\n",
      "Setting up libcups2:amd64 (2.4.1op1-1ubuntu4.11) ...\n",
      "Setting up ca-certificates-java (20190909ubuntu1.2) ...\n",
      "head: cannot open '/etc/ssl/certs/java/cacerts' for reading: No such file or directory\n",
      "Adding debian:ACCVRAIZ1.pem\n",
      "Adding debian:AC_RAIZ_FNMT-RCM.pem\n",
      "Adding debian:AC_RAIZ_FNMT-RCM_SERVIDORES_SEGUROS.pem\n",
      "Adding debian:ANF_Secure_Server_Root_CA.pem\n",
      "Adding debian:Actalis_Authentication_Root_CA.pem\n",
      "Adding debian:AffirmTrust_Commercial.pem\n",
      "Adding debian:AffirmTrust_Networking.pem\n",
      "Adding debian:AffirmTrust_Premium.pem\n",
      "Adding debian:AffirmTrust_Premium_ECC.pem\n",
      "Adding debian:Amazon_Root_CA_1.pem\n",
      "Adding debian:Amazon_Root_CA_2.pem\n",
      "Adding debian:Amazon_Root_CA_3.pem\n",
      "Adding debian:Amazon_Root_CA_4.pem\n",
      "Adding debian:Atos_TrustedRoot_2011.pem\n",
      "Adding debian:Autoridad_de_Certificacion_Firmaprofesional_CIF_A62634068.pem\n",
      "Adding debian:Autoridad_de_Certificacion_Firmaprofesional_CIF_A62634068_2.pem\n",
      "Adding debian:Baltimore_CyberTrust_Root.pem\n",
      "Adding debian:Buypass_Class_2_Root_CA.pem\n",
      "Adding debian:Buypass_Class_3_Root_CA.pem\n",
      "Adding debian:CA_Disig_Root_R2.pem\n",
      "Adding debian:CFCA_EV_ROOT.pem\n",
      "Adding debian:COMODO_Certification_Authority.pem\n",
      "Adding debian:COMODO_ECC_Certification_Authority.pem\n",
      "Adding debian:COMODO_RSA_Certification_Authority.pem\n",
      "Adding debian:Certainly_Root_E1.pem\n",
      "Adding debian:Certainly_Root_R1.pem\n",
      "Adding debian:Certigna.pem\n",
      "Adding debian:Certigna_Root_CA.pem\n",
      "Adding debian:Certum_EC-384_CA.pem\n",
      "Adding debian:Certum_Trusted_Network_CA.pem\n",
      "Adding debian:Certum_Trusted_Network_CA_2.pem\n",
      "Adding debian:Certum_Trusted_Root_CA.pem\n",
      "Adding debian:Comodo_AAA_Services_root.pem\n",
      "Adding debian:D-TRUST_BR_Root_CA_1_2020.pem\n",
      "Adding debian:D-TRUST_EV_Root_CA_1_2020.pem\n",
      "Adding debian:D-TRUST_Root_Class_3_CA_2_2009.pem\n",
      "Adding debian:D-TRUST_Root_Class_3_CA_2_EV_2009.pem\n",
      "Adding debian:DigiCert_Assured_ID_Root_CA.pem\n",
      "Adding debian:DigiCert_Assured_ID_Root_G2.pem\n",
      "Adding debian:DigiCert_Assured_ID_Root_G3.pem\n",
      "Adding debian:DigiCert_Global_Root_CA.pem\n",
      "Adding debian:DigiCert_Global_Root_G2.pem\n",
      "Adding debian:DigiCert_Global_Root_G3.pem\n",
      "Adding debian:DigiCert_High_Assurance_EV_Root_CA.pem\n",
      "Adding debian:DigiCert_TLS_ECC_P384_Root_G5.pem\n",
      "Adding debian:DigiCert_TLS_RSA4096_Root_G5.pem\n",
      "Adding debian:DigiCert_Trusted_Root_G4.pem\n",
      "Adding debian:E-Tugra_Certification_Authority.pem\n",
      "Adding debian:E-Tugra_Global_Root_CA_ECC_v3.pem\n",
      "Adding debian:E-Tugra_Global_Root_CA_RSA_v3.pem\n",
      "Adding debian:Entrust.net_Premium_2048_Secure_Server_CA.pem\n",
      "Adding debian:Entrust_Root_Certification_Authority.pem\n",
      "Adding debian:Entrust_Root_Certification_Authority_-_EC1.pem\n",
      "Adding debian:Entrust_Root_Certification_Authority_-_G2.pem\n",
      "Adding debian:Entrust_Root_Certification_Authority_-_G4.pem\n",
      "Adding debian:GDCA_TrustAUTH_R5_ROOT.pem\n",
      "Adding debian:GLOBALTRUST_2020.pem\n",
      "Adding debian:GTS_Root_R1.pem\n",
      "Adding debian:GTS_Root_R2.pem\n",
      "Adding debian:GTS_Root_R3.pem\n",
      "Adding debian:GTS_Root_R4.pem\n",
      "Adding debian:GlobalSign_ECC_Root_CA_-_R4.pem\n",
      "Adding debian:GlobalSign_ECC_Root_CA_-_R5.pem\n",
      "Adding debian:GlobalSign_Root_CA.pem\n",
      "Adding debian:GlobalSign_Root_CA_-_R3.pem\n",
      "Adding debian:GlobalSign_Root_CA_-_R6.pem\n",
      "Adding debian:GlobalSign_Root_E46.pem\n",
      "Adding debian:GlobalSign_Root_R46.pem\n",
      "Adding debian:Go_Daddy_Class_2_CA.pem\n",
      "Adding debian:Go_Daddy_Root_Certificate_Authority_-_G2.pem\n",
      "Adding debian:HARICA_TLS_ECC_Root_CA_2021.pem\n",
      "Adding debian:HARICA_TLS_RSA_Root_CA_2021.pem\n",
      "Adding debian:Hellenic_Academic_and_Research_Institutions_ECC_RootCA_2015.pem\n",
      "Adding debian:Hellenic_Academic_and_Research_Institutions_RootCA_2015.pem\n",
      "Adding debian:HiPKI_Root_CA_-_G1.pem\n",
      "Adding debian:Hongkong_Post_Root_CA_1.pem\n",
      "Adding debian:Hongkong_Post_Root_CA_3.pem\n",
      "Adding debian:ISRG_Root_X1.pem\n",
      "Adding debian:ISRG_Root_X2.pem\n",
      "Adding debian:IdenTrust_Commercial_Root_CA_1.pem\n",
      "Adding debian:IdenTrust_Public_Sector_Root_CA_1.pem\n",
      "Adding debian:Izenpe.com.pem\n",
      "Adding debian:Microsec_e-Szigno_Root_CA_2009.pem\n",
      "Adding debian:Microsoft_ECC_Root_Certificate_Authority_2017.pem\n",
      "Adding debian:Microsoft_RSA_Root_Certificate_Authority_2017.pem\n",
      "Adding debian:NAVER_Global_Root_Certification_Authority.pem\n",
      "Adding debian:NetLock_Arany_=Class_Gold=_Főtanúsítvány.pem\n",
      "Adding debian:OISTE_WISeKey_Global_Root_GB_CA.pem\n",
      "Adding debian:OISTE_WISeKey_Global_Root_GC_CA.pem\n",
      "Adding debian:QuoVadis_Root_CA_1_G3.pem\n",
      "Adding debian:QuoVadis_Root_CA_2.pem\n",
      "Adding debian:QuoVadis_Root_CA_2_G3.pem\n",
      "Adding debian:QuoVadis_Root_CA_3.pem\n",
      "Adding debian:QuoVadis_Root_CA_3_G3.pem\n",
      "Adding debian:SSL.com_EV_Root_Certification_Authority_ECC.pem\n",
      "Adding debian:SSL.com_EV_Root_Certification_Authority_RSA_R2.pem\n",
      "Adding debian:SSL.com_Root_Certification_Authority_ECC.pem\n",
      "Adding debian:SSL.com_Root_Certification_Authority_RSA.pem\n",
      "Adding debian:SZAFIR_ROOT_CA2.pem\n",
      "Adding debian:SecureSign_RootCA11.pem\n",
      "Adding debian:SecureTrust_CA.pem\n",
      "Adding debian:Secure_Global_CA.pem\n",
      "Adding debian:Security_Communication_ECC_RootCA1.pem\n",
      "Adding debian:Security_Communication_RootCA2.pem\n",
      "Adding debian:Security_Communication_RootCA3.pem\n",
      "Adding debian:Security_Communication_Root_CA.pem\n",
      "Adding debian:Starfield_Class_2_CA.pem\n",
      "Adding debian:Starfield_Root_Certificate_Authority_-_G2.pem\n",
      "Adding debian:Starfield_Services_Root_Certificate_Authority_-_G2.pem\n",
      "Adding debian:SwissSign_Gold_CA_-_G2.pem\n",
      "Adding debian:SwissSign_Silver_CA_-_G2.pem\n",
      "Adding debian:T-TeleSec_GlobalRoot_Class_2.pem\n",
      "Adding debian:T-TeleSec_GlobalRoot_Class_3.pem\n",
      "Adding debian:TUBITAK_Kamu_SM_SSL_Kok_Sertifikasi_-_Surum_1.pem\n",
      "Adding debian:TWCA_Global_Root_CA.pem\n",
      "Adding debian:TWCA_Root_Certification_Authority.pem\n",
      "Adding debian:TeliaSonera_Root_CA_v1.pem\n",
      "Adding debian:Telia_Root_CA_v2.pem\n",
      "Adding debian:Trustwave_Global_Certification_Authority.pem\n",
      "Adding debian:Trustwave_Global_ECC_P256_Certification_Authority.pem\n",
      "Adding debian:Trustwave_Global_ECC_P384_Certification_Authority.pem\n",
      "Adding debian:TunTrust_Root_CA.pem\n",
      "Adding debian:UCA_Extended_Validation_Root.pem\n",
      "Adding debian:UCA_Global_G2_Root.pem\n",
      "Adding debian:USERTrust_ECC_Certification_Authority.pem\n",
      "Adding debian:USERTrust_RSA_Certification_Authority.pem\n",
      "Adding debian:XRamp_Global_CA_Root.pem\n",
      "Adding debian:certSIGN_ROOT_CA.pem\n",
      "Adding debian:certSIGN_Root_CA_G2.pem\n",
      "Adding debian:e-Szigno_Root_CA_2017.pem\n",
      "Adding debian:ePKI_Root_Certification_Authority.pem\n",
      "Adding debian:emSign_ECC_Root_CA_-_C3.pem\n",
      "Adding debian:emSign_ECC_Root_CA_-_G3.pem\n",
      "Adding debian:emSign_Root_CA_-_C1.pem\n",
      "Adding debian:emSign_Root_CA_-_G1.pem\n",
      "Adding debian:vTrus_ECC_Root_CA.pem\n",
      "Adding debian:vTrus_Root_CA.pem\n",
      "done.\n",
      "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
      "Processing triggers for ca-certificates (20230311ubuntu0.22.04.1) ...\n",
      "Updating certificates in /etc/ssl/certs...\n",
      "0 added, 0 removed; done.\n",
      "Running hooks in /etc/ca-certificates/update.d...\n",
      "\n",
      "done.\n",
      "done.\n",
      "Setting up openjdk-17-jre-headless:amd64 (17.0.15+6~us1-0ubuntu1~22.04) ...\n",
      "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/java to provide /usr/bin/java (java) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/jpackage to provide /usr/bin/jpackage (jpackage) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/keytool to provide /usr/bin/keytool (keytool) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/rmiregistry to provide /usr/bin/rmiregistry (rmiregistry) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/lib/jexec to provide /usr/bin/jexec (jexec) in auto mode\n"
     ]
    }
   ],
   "source": [
    "!apt-get update -y\n",
    "!apt-get install -y openjdk-17-jre-headless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "754e3a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-03 06:19:53.310670: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-03 06:19:53.321143: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1751523593.334046    2631 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1751523593.338048    2631 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1751523593.348615    2631 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751523593.348635    2631 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751523593.348637    2631 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751523593.348638    2631 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-03 06:19:53.352218: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from transformers import get_scheduler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from custom_tokenizers import Tokenizer\n",
    "from configs.config import DataConfig, EncoderConfig, DecoderConfig, PaliGemmaConfig\n",
    "from decoder_layers import KVCache, PaliGemmaForConditionalGeneration\n",
    "from dataloaders import CustomDataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf9671bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model,\n",
    "    dataloader,\n",
    "    optimizer,\n",
    "    device,\n",
    "    epoch,\n",
    "    grad_accumulation_steps=1,\n",
    "    max_grad_norm=1.0,\n",
    "    use_amp=False,\n",
    "):\n",
    "    model.train()\n",
    "    kv_cache = KVCache()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "    config = DecoderConfig()\n",
    "\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for step, batch in enumerate(tqdm(dataloader, desc=f\"Epoch {epoch}\")):\n",
    "        input_ids, pixel_values, attention_mask = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        pixel_values = pixel_values.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                pixel_values=pixel_values,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            \n",
    "            logits = outputs[\"logits\"]\n",
    "            shift_logits = logits[:, :-1, :].contiguous()\n",
    "            shift_labels = input_ids[:, 1:].contiguous()\n",
    "            \n",
    "            loss = torch.nn.functional.cross_entropy(\n",
    "                shift_logits.view(-1, config.vocab_size),\n",
    "                shift_labels.view(-1),\n",
    "                label_smoothing=0.1\n",
    "            )\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if (step + 1) % grad_accumulation_steps == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, dataloader, device, use_amp=False):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "    config = DecoderConfig()\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Validating\"):\n",
    "        input_ids, pixel_values, attention_mask = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        pixel_values = pixel_values.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                pixel_values=pixel_values,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            logits = outputs[\"logits\"]  # shape: [B, T, V]\n",
    "            shift_logits = logits[:, :-1, :].contiguous()\n",
    "            shift_labels = input_ids[:, 1:].contiguous()\n",
    "            \n",
    "            loss = torch.nn.functional.cross_entropy(\n",
    "                shift_logits.view(-1, config.vocab_size),\n",
    "                shift_labels.view(-1),\n",
    "                label_smoothing=0.1\n",
    "            )\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58d00b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Hyperparameters\n",
    "    epochs = 10\n",
    "    batch_size = 64\n",
    "    learning_rate = 3e-5\n",
    "    grad_accumulation_steps = 1\n",
    "    use_amp = True\n",
    "    patience = 30\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    # === Load Config ===\n",
    "    text_config = DecoderConfig()\n",
    "    vision_config = EncoderConfig()\n",
    "    text_config.vocab_size += 1\n",
    "    image_token_index = text_config.vocab_size - 1\n",
    "    config = PaliGemmaConfig(\n",
    "        text_config=text_config,\n",
    "        vision_config=vision_config,\n",
    "        image_token_index=image_token_index,\n",
    "    )\n",
    "\n",
    "    # === Load Model ===\n",
    "    model = PaliGemmaForConditionalGeneration(config).to(device)\n",
    "\n",
    "    # Tokenizer\n",
    "    tokenizer = Tokenizer(DataConfig())\n",
    "\n",
    "    # === Dataloader ===\n",
    "    train_loader = CustomDataLoader(\n",
    "        split=\"train\",\n",
    "        batch_size=batch_size,\n",
    "        num_workers=1,\n",
    "        tokenizer=tokenizer,\n",
    "        shuffle=True\n",
    "    )\n",
    "    val_loader = CustomDataLoader(\n",
    "        split=\"val\",\n",
    "        batch_size=batch_size,\n",
    "        num_workers=1,\n",
    "        tokenizer=tokenizer,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    # === Optimizer and Scheduler ===\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    num_training_steps = epochs * len(train_loader) // grad_accumulation_steps\n",
    "    # lr_scheduler = get_scheduler(\n",
    "    #     \"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    "    # )\n",
    "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, threshold=0.0001, threshold_mode='abs')\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "\n",
    "    # === Training Loop ===\n",
    "    start_time = time.time()\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        avg_train_loss = train(\n",
    "            model,\n",
    "            train_loader,\n",
    "            optimizer,\n",
    "            # lr_scheduler,\n",
    "            device,\n",
    "            epoch,\n",
    "            grad_accumulation_steps,\n",
    "            use_amp=use_amp,\n",
    "        )\n",
    "        avg_val_loss = validate(model, val_loader, device, use_amp)\n",
    "        lr_scheduler.step(avg_val_loss)\n",
    "        train_loss.append(avg_train_loss)\n",
    "        val_loss.append(avg_val_loss)\n",
    "        print(f\"Epoch {epoch} - Train Loss: {avg_train_loss:.4f} - Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            print(\"Validation loss improved. Saving model...\")\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "            torch.save(model.state_dict(), f\"checkpoints/experiment_6.pt\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"No improvement. Patience: {patience_counter}/{patience}\")\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "    save_dir = \"plot\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Plotting\n",
    "    epochs = list(range(1, len(train_loss) + 1))\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(epochs, train_loss, label='Train Loss', marker='o')\n",
    "    plt.plot(epochs, val_loss, label='Validation Loss', marker='x')\n",
    "    plt.title(\"Training and Validation Loss per Epoch\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Save the plot\n",
    "    plot_path = os.path.join(save_dir, \"experiment_6.png\")\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()\n",
    "\n",
    "    # Calculate elapsed time\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Elapsed time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a817e6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 33/33 [00:35<00:00,  1.06s/it]\n",
      "Validating: 100%|██████████| 5/5 [00:05<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train Loss: 3.8154 - Val Loss: 2.3447\n",
      "Validation loss improved. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 33/33 [00:34<00:00,  1.04s/it]\n",
      "Validating: 100%|██████████| 5/5 [00:05<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Train Loss: 2.0775 - Val Loss: 1.9575\n",
      "Validation loss improved. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 33/33 [00:34<00:00,  1.05s/it]\n",
      "Validating: 100%|██████████| 5/5 [00:05<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Train Loss: 1.8767 - Val Loss: 1.7503\n",
      "Validation loss improved. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 33/33 [00:34<00:00,  1.05s/it]\n",
      "Validating: 100%|██████████| 5/5 [00:05<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Train Loss: 1.6936 - Val Loss: 1.6201\n",
      "Validation loss improved. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 33/33 [00:34<00:00,  1.05s/it]\n",
      "Validating: 100%|██████████| 5/5 [00:05<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Train Loss: 1.5928 - Val Loss: 1.5533\n",
      "Validation loss improved. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 33/33 [00:34<00:00,  1.03s/it]\n",
      "Validating: 100%|██████████| 5/5 [00:05<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Train Loss: 1.5428 - Val Loss: 1.5148\n",
      "Validation loss improved. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 33/33 [00:34<00:00,  1.04s/it]\n",
      "Validating: 100%|██████████| 5/5 [00:05<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Train Loss: 1.5102 - Val Loss: 1.4876\n",
      "Validation loss improved. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 33/33 [00:34<00:00,  1.04s/it]\n",
      "Validating: 100%|██████████| 5/5 [00:05<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Train Loss: 1.4866 - Val Loss: 1.4668\n",
      "Validation loss improved. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 33/33 [00:34<00:00,  1.04s/it]\n",
      "Validating: 100%|██████████| 5/5 [00:05<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Train Loss: 1.4655 - Val Loss: 1.4497\n",
      "Validation loss improved. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 33/33 [00:33<00:00,  1.02s/it]\n",
      "Validating: 100%|██████████| 5/5 [00:05<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Train Loss: 1.4489 - Val Loss: 1.4359\n",
      "Validation loss improved. Saving model...\n",
      "Elapsed time: 416.77386713027954 seconds\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87a85d1d-bcc9-4093-901b-02f080ddc7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import clip\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "from dataloaders import CustomDataLoader\n",
    "from custom_tokenizers import Tokenizer\n",
    "from types import SimpleNamespace\n",
    "from decoder_layers import KVCache, PaliGemmaForConditionalGeneration\n",
    "from configs.config import DataConfig, EncoderConfig, DecoderConfig, PaliGemmaConfig\n",
    "from metrics import compute_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "730b2268-52b6-4b00-a221-e13c9a67cc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def greedy_generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    pixel_values,        # shape: (B, 2, C, H, W)\n",
    "    image_token_index,   # placeholder index for image tokens\n",
    "    max_length=60,\n",
    "    eos_token_id=2,\n",
    "    pad_token_id=0,\n",
    "    device=\"cuda\",\n",
    "):\n",
    "    model.eval()\n",
    "    batch_size = pixel_values.size(0)\n",
    "    eos_token_id = eos_token_id or tokenizer.eos_token_id\n",
    "\n",
    "    # Encode images\n",
    "    B, N, C, H, W = pixel_values.shape\n",
    "    pixel_values = pixel_values.view(B * N, C, H, W).to(device)\n",
    "    pixel_values = pixel_values.to(dtype=next(model.vision_tower.parameters()).dtype, device=device)\n",
    "    vision_features = model.vision_tower(pixel_values)\n",
    "    vision_features = vision_features.view(B, N, *vision_features.shape[1:])\n",
    "    image_features = torch.cat([vision_features[:, 0], vision_features[:, 1]], dim=1)\n",
    "    image_features = model.multi_modal_projector(image_features)  # shape: [B, Seq, Hidden]\n",
    "\n",
    "    # Initialize sequence\n",
    "    input_ids = torch.full((batch_size, 1), image_token_index, dtype=torch.long, device=device)\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "\n",
    "    # Initialize done mask\n",
    "    is_done = torch.zeros(batch_size, dtype=torch.bool, device=device)\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        # Embedding\n",
    "        input_embeds = model.language_model.get_input_embeddings()(input_ids)\n",
    "\n",
    "        # Merge image and text\n",
    "        merged_input, attn_mask, pos_ids = model._merge_input_ids_with_image_features(\n",
    "            input_ids=input_ids,\n",
    "            image_features=image_features,\n",
    "            inputs_embeds=input_embeds,\n",
    "            attention_mask=attention_mask,\n",
    "            kv_cache=None\n",
    "        )\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model.language_model(\n",
    "            inputs_embeds=merged_input,\n",
    "            attention_mask=attn_mask,\n",
    "            position_ids=pos_ids,\n",
    "        )\n",
    "\n",
    "        logits = outputs[\"logits\"]\n",
    "        next_token_logits = logits[:, -1, :]  # Last token logits\n",
    "        next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(1)  # Greedy pick\n",
    "\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "        attention_mask = torch.cat([attention_mask, torch.ones_like(next_token)], dim=1)\n",
    "\n",
    "        # Mark sequences that have ended\n",
    "        is_done = is_done | (next_token.squeeze(1) == eos_token_id)\n",
    "        if is_done.all():\n",
    "            break\n",
    "\n",
    "    return input_ids\n",
    "\n",
    "def beam_search_generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    pixel_values,        # shape: (B, 2, C, H, W)\n",
    "    image_token_index,   # placeholder index for image tokens\n",
    "    max_length=60,\n",
    "    num_beams=5,\n",
    "    eos_token_id=2,\n",
    "    pad_token_id=0,\n",
    "    device=\"cuda\",\n",
    "):\n",
    "    model.eval()\n",
    "    batch_size = pixel_values.size(0)\n",
    "    eos_token_id = eos_token_id or tokenizer.eos_token_id\n",
    "\n",
    "    # Encode images\n",
    "    B, N, C, H, W = pixel_values.shape\n",
    "    pixel_values = pixel_values.view(B * N, C, H, W).to(device)\n",
    "    pixel_values = pixel_values.to(dtype=next(model.vision_tower.parameters()).dtype, device=\"cuda\")\n",
    "    vision_features = model.vision_tower(pixel_values)\n",
    "    vision_features = vision_features.view(B, N, *vision_features.shape[1:])\n",
    "    image_features = torch.cat([vision_features[:, 0], vision_features[:, 1]], dim=1)\n",
    "    image_features = model.multi_modal_projector(image_features)  # shape: [B, Seq, Hidden]\n",
    "\n",
    "    # Init input_ids with image token index\n",
    "    input_ids = torch.full((batch_size * num_beams, 1), image_token_index, dtype=torch.long, device=device)\n",
    "    attention_mask = torch.ones_like(input_ids, device=device)\n",
    "\n",
    "    # Expand inputs for each beam\n",
    "    image_features = image_features.unsqueeze(1).repeat(1, num_beams, 1, 1)\n",
    "    image_features = image_features.view(batch_size * num_beams, *image_features.shape[2:])\n",
    "\n",
    "    beam_scores = torch.zeros((batch_size, num_beams), device=device)\n",
    "    beam_scores[:, 1:] = -1e9  # mask beams other than first\n",
    "    beam_scores = beam_scores.view(-1)  # shape: [B * num_beams]\n",
    "\n",
    "    sequences = input_ids\n",
    "    is_done = [False] * batch_size\n",
    "\n",
    "    for step in range(max_length):\n",
    "        # Embedding\n",
    "        input_embeds = model.language_model.get_input_embeddings()(sequences)\n",
    "        \n",
    "        # Merge image + text features\n",
    "        merged_input, attn_mask, pos_ids = model._merge_input_ids_with_image_features(\n",
    "            input_ids = sequences, image_features = image_features, inputs_embeds = input_embeds, attention_mask = attention_mask, kv_cache=None\n",
    "        )\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model.language_model(\n",
    "            inputs_embeds=merged_input,\n",
    "            attention_mask=attn_mask,\n",
    "            position_ids=pos_ids,\n",
    "        )\n",
    "        logits = outputs[\"logits\"]  # shape: [B * num_beams, Seq_len, Vocab]\n",
    "        next_token_logits = logits[:, -1, :]  # take last token only\n",
    "        next_token_log_probs = F.log_softmax(next_token_logits, dim=-1)\n",
    "        # next_token_log_probs[:, image_token_index] = -1e9\n",
    "\n",
    "\n",
    "        # Add current beam scores\n",
    "        next_token_log_probs = next_token_log_probs + beam_scores[:, None]\n",
    "\n",
    "        # Get top k * num_beams candidates\n",
    "        vocab_size = next_token_log_probs.size(-1)\n",
    "        next_token_log_probs = next_token_log_probs.view(batch_size, num_beams * vocab_size)\n",
    "        topk_log_probs, topk_indices = torch.topk(next_token_log_probs, num_beams, dim=-1)\n",
    "\n",
    "        # Prepare for next step\n",
    "        beam_indices = topk_indices // vocab_size\n",
    "        token_indices = topk_indices % vocab_size\n",
    "\n",
    "        # Reorder sequences and image features\n",
    "        sequences = sequences.view(batch_size, num_beams, -1)\n",
    "        new_sequences = []\n",
    "        for i in range(batch_size):\n",
    "            new_sequences.append(sequences[i, beam_indices[i]])\n",
    "        sequences = torch.stack(new_sequences).view(batch_size * num_beams, -1)\n",
    "        sequences = torch.cat([sequences, token_indices.view(-1, 1)], dim=-1)\n",
    "\n",
    "        # Update scores\n",
    "        beam_scores = topk_log_probs.view(-1)\n",
    "\n",
    "        # Update attention mask\n",
    "        attention_mask = torch.cat([attention_mask, torch.ones_like(token_indices.view(-1, 1))], dim=1)\n",
    "\n",
    "        # Check if all sequences have ended\n",
    "        if eos_token_id is not None:\n",
    "            for i in range(batch_size):\n",
    "                done_for_beam = True\n",
    "                for beam_id in range(num_beams):\n",
    "                    token = sequences[i * num_beams + beam_id, -1]\n",
    "                    if token != eos_token_id:\n",
    "                        done_for_beam = False\n",
    "                        break\n",
    "                is_done[i] = done_for_beam\n",
    "        \n",
    "            if all(is_done):\n",
    "                break\n",
    "\n",
    "    # Reshape to [batch_size, num_beams, seq_len] and pick best beam\n",
    "    sequences = sequences.view(batch_size, num_beams, -1)\n",
    "    beam_scores = beam_scores.view(batch_size, num_beams)\n",
    "    best_indices = torch.argmax(beam_scores, dim=1)\n",
    "\n",
    "    best_sequences = []\n",
    "    for i in range(batch_size):\n",
    "        best_sequences.append(sequences[i, best_indices[i]])\n",
    "    best_sequences = torch.stack(best_sequences)\n",
    "\n",
    "    return best_sequences\n",
    "\n",
    "\n",
    "def evaluate_model(model, tokenizer, dataloader, device, image_token_index, decoding, max_len=60, num_beams=5):\n",
    "    model.eval()\n",
    "    gts = {}\n",
    "    res = {}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tqdm(dataloader, desc=\"Evaluating\")):\n",
    "            input_ids, pixel_values, att_masks = batch\n",
    "            pixel_values = pixel_values.to(device)\n",
    "\n",
    "            if decoding == \"greedy\":\n",
    "                print(\"generate with greedy\")\n",
    "                generated_ids = greedy_generate(\n",
    "                    model,\n",
    "                    tokenizer,\n",
    "                    pixel_values=pixel_values,\n",
    "                    image_token_index=image_token_index,\n",
    "                    eos_token_id=tokenizer.eos_token_id,\n",
    "                    device=device,\n",
    "                    max_length=max_len,\n",
    "                )\n",
    "            else:\n",
    "                print(\"generate with beam\")\n",
    "                generated_ids = beam_search_generate(\n",
    "                    model,\n",
    "                    tokenizer,\n",
    "                    pixel_values=pixel_values,\n",
    "                    image_token_index=image_token_index,\n",
    "                    eos_token_id=tokenizer.eos_token_id,\n",
    "                    device=device,\n",
    "                    num_beams=num_beams,\n",
    "                    max_length=max_len,\n",
    "                )\n",
    "\n",
    "            # Decode predictions\n",
    "            decoded_preds = tokenizer.decode_batch(generated_ids)\n",
    "            decoded_preds = [pred.strip().lower() for pred in decoded_preds]\n",
    "\n",
    "            # Decode references\n",
    "            references = input_ids\n",
    "            if isinstance(references[0], torch.Tensor):\n",
    "                references = [tokenizer.decode(ref).replace(\"<image>\", \"\").strip().lower() for ref in references]\n",
    "\n",
    "            batch_size = pixel_values.size(0)\n",
    "            for j in range(batch_size):\n",
    "                image_id = f\"img_{i * batch_size + j}\"\n",
    "                gts[image_id] = [references[j]]  # list of refs\n",
    "                res[image_id] = [decoded_preds[j]]  # model output\n",
    "            for i in range(1):\n",
    "                print(f\"Pred: {decoded_preds[i]}\")\n",
    "                print(f\"Ref: {references[i]}\")\n",
    "                print()\n",
    "\n",
    "    scores = compute_scores(gts, res)\n",
    "\n",
    "    # Optional print\n",
    "    for metric, score in scores.items():\n",
    "        print(f\"{metric.upper()}: {score:.4f}\")\n",
    "\n",
    "    return scores, gts, res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55193c40-a4d9-41e5-b753-0fcc98af0f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 338M/338M [00:02<00:00, 167MiB/s]\n",
      "Evaluating:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate with greedy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  10%|█         | 1/10 [00:03<00:28,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred: <image> 5th acute bony structures reveal masses replacement exam most compatible with tip projects over the cardiac silhouette is a small t-spine osteophytes changes throughout both lungs are clear bilaterally more this could represent sequelae <bos> frontal view reveals granulomas throughout both lungs are clear bilaterally more this could represent sequelae <bos> frontal view reveals granulomas throughout both lungs are clear\n",
      "Ref: <bos> unchanged cardiomegaly . there is <unk> interstitial prominence bilaterally . unchanged vascular appearance . there is patchy retrocardiac opacity . negative for pneumothorax . <eos>\n",
      "\n",
      "generate with greedy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  20%|██        | 2/10 [00:04<00:16,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred: <image> normally inflated without evidence of frontal view reveals granulomas throughout both lungs are clear bilaterally more this could represent sequelae <bos> frontal view reveals granulomas throughout both lungs are clear bilaterally more this could represent sequelae <bos> frontal view reveals granulomas throughout both lungs are clear bilaterally more this could represent sequelae <bos> frontal view reveals granulomas throughout both lungs\n",
      "Ref: <bos> the cardiomediastinal silhouette is within normal limits . there is rounded calcified density within the left lower lobe most consistent with granuloma . <unk> lungs are clear without evidence of focal opacification . no pneumothorax or large pleural effusion . no acute bone abnormality . <eos>\n",
      "\n",
      "generate with greedy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  30%|███       | 3/10 [00:05<00:12,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred: <image> normally inflated without evidence of frontal view reveals granulomas throughout both lungs are clear bilaterally more this could represent sequelae <bos> frontal view reveals granulomas throughout both lungs are clear bilaterally more this could represent sequelae <bos> frontal view reveals granulomas throughout both lungs are clear bilaterally more this could represent sequelae <bos> frontal view reveals granulomas throughout both lungs\n",
      "Ref: <bos> no focal lung consolidation . heart size and pulmonary vascularity are within normal limits . no pneumothorax or pleural effusion . osseous structures are grossly intact . <eos>\n",
      "\n",
      "generate with greedy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  40%|████      | 4/10 [00:07<00:09,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred: <image> normally inflated without evidence of frontal view reveals granulomas throughout both lungs are clear bilaterally more this could represent sequelae <bos> frontal view reveals granulomas throughout both lungs are clear bilaterally more this could represent sequelae <bos> frontal view reveals granulomas throughout both lungs are clear bilaterally more this could represent sequelae <bos> frontal view reveals granulomas throughout both lungs\n",
      "Ref: <bos> cardiac and mediastinal contours are within normal limits . the lungs are clear . bony structures are intact . <eos>\n",
      "\n",
      "generate with greedy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  50%|█████     | 5/10 [00:08<00:07,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred: <image> normally inflated without evidence of frontal view reveals granulomas throughout both lungs are clear bilaterally more images apices could represent sequelae <bos> frontal view reveals granulomas throughout both lungs are clear bilaterally more this could represent sequelae <bos> frontal view reveals granulomas throughout both lungs are clear bilaterally more this could represent sequelae <bos> frontal view reveals granulomas throughout both\n",
      "Ref: <bos> heart size is normal . there are xxxx opacities which appear to xxxx xxxx above the right xxxx fissure . there is mild thickening in the fissure . no pneumothorax . no large pleural effusions . <eos>\n",
      "\n",
      "generate with greedy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  60%|██████    | 6/10 [00:09<00:05,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred: <image> normally inflated without evidence of frontal view reveals granulomas throughout both lungs are clear bilaterally more this could represent sequelae <bos> frontal view reveals granulomas throughout both lungs are clear bilaterally more this could represent sequelae <bos> frontal view reveals granulomas throughout both lungs are clear bilaterally more this could represent sequelae <bos> frontal view reveals granulomas throughout both lungs\n",
      "Ref: <bos> mediastinal contours are normal . lungs are clear . there is no pneumothorax or large pleural effusion . <eos>\n",
      "\n",
      "generate with greedy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  70%|███████   | 7/10 [00:11<00:04,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred: <image> normally inflated without evidence of frontal view reveals granulomas throughout both lungs are clear bilaterally more this could represent sequelae <bos> frontal view reveals granulomas throughout both lungs are clear bilaterally more this could represent sequelae <bos> frontal view reveals granulomas throughout both lungs are clear bilaterally more this could represent sequelae <bos> frontal view reveals granulomas throughout both lungs\n",
      "Ref: <bos> the heart is normal in size . the mediastinum is unremarkable . the lungs are clear . <eos>\n",
      "\n",
      "generate with greedy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  80%|████████  | 8/10 [00:12<00:02,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred: <image> normally inflated without evidence of frontal view reveals granulomas throughout both lungs are clear bilaterally more this could represent sequelae <bos> frontal view reveals granulomas throughout both lungs are clear bilaterally more this could represent sequelae <bos> frontal view reveals granulomas throughout both lungs are clear bilaterally more this could represent sequelae <bos> frontal view reveals granulomas throughout both lungs\n",
      "Ref: <bos> the cardiac contours are normal . the lungs are clear . thoracic spondylosis . <eos>\n",
      "\n",
      "generate with greedy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  90%|█████████ | 9/10 [00:13<00:01,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred: <image> normally inflated without evidence of frontal view reveals granulomas throughout both lungs are clear bilaterally more this could represent sequelae <bos> frontal view reveals granulomas throughout both lungs are clear bilaterally more this could represent sequelae <bos> frontal view reveals granulomas throughout both lungs are clear bilaterally more this could represent sequelae <bos> frontal view reveals granulomas throughout both lungs\n",
      "Ref: <bos> the heart pulmonary xxxx and mediastinum are within normal limits . there is no pleural effusion or pneumothorax . there is no focal air space opacity to suggest a pneumonia . <eos>\n",
      "\n",
      "generate with greedy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 10/10 [00:14<00:00,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred: <image> <unk> cm nodular density projected over the cardiac silhouette is a small t-spine osteophytes changes throughout both lungs are clear bilaterally more this could represent sequelae <bos> frontal view reveals granulomas throughout both lungs are clear bilaterally more this could represent sequelae <bos> frontal view reveals granulomas throughout both lungs are clear bilaterally more images apices could represent sequelae <bos>\n",
      "Ref: <bos> normal heart size and mediastinal contours . stable calcification in the left upper lobe xxxx representing a granuloma . no focal airspace opacities . no pleural effusion or pneumothorax . visualized osseous structures are unremarkable in appearance . <eos>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU_1: 0.0719\n",
      "BLEU_2: 0.0388\n",
      "BLEU_3: 0.0240\n",
      "BLEU_4: 0.0132\n",
      "METEOR: 0.1057\n",
      "ROUGE_L: 0.0840\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'BLEU_1': 0.0718856078584496,\n",
       " 'BLEU_2': 0.03876937847659147,\n",
       " 'BLEU_3': 0.024038623641290346,\n",
       " 'BLEU_4': 0.013248570457362201,\n",
       " 'METEOR': 0.1057116640117974,\n",
       " 'ROUGE_L': np.float64(0.08402614183632913)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = PaliGemmaConfig(text_config=DecoderConfig(), vision_config=EncoderConfig())\n",
    "model = PaliGemmaForConditionalGeneration(config)\n",
    "model.load_state_dict(torch.load(\"checkpoints/experiment_2.pt\", map_location=\"cuda\"))\n",
    "model.eval()\n",
    "\n",
    "tokenizer = Tokenizer(DataConfig())\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_clip, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "model = model.to(device)\n",
    "\n",
    "tokenizer = Tokenizer(DataConfig())\n",
    "\n",
    "test_loader = CustomDataLoader(\n",
    "    split=\"test\",\n",
    "    batch_size=64,\n",
    "    num_workers=1,\n",
    "    tokenizer=tokenizer,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "scores, gts, res = evaluate_model(model, tokenizer, test_loader, device, image_token_index=763, decoding=\"greedy\")\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a7de3af-4c8e-4880-a14f-2a466ea3bdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"gts_greed.json\", \"w\") as file:\n",
    "    json.dump(gts, file)\n",
    "with open(\"res_greed.json\", \"w\") as file:\n",
    "    json.dump(res, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
